{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd8c8bc-a7c0-47e9-9179-234174167325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCMVAE: 100%|██████████| 150/150 [00:01<00:00, 142.12it/s]\n",
      "R-CRN: 100%|██████████| 100/100 [00:00<00:00, 276.78it/s]\n",
      "CF-RNN: 100%|██████████| 100/100 [00:00<00:00, 273.32it/s]\n",
      "TS-TARNet: 100%|██████████| 100/100 [00:00<00:00, 434.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model           | Test RMSE    | Test PEHE    | Test ATE    \n",
      "----------------------------------------------------------------------\n",
      "DCMVAE          | 0.3206       | 3.8390       | 0.9700\n",
      "R-CRN           | 0.2079       | 3.8548       | 0.9160\n",
      "CF-RNN          | 0.2242       | 3.8663       | 0.9597\n",
      "TS-TARNet       | 0.2031       | 3.8582       | 0.9279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 0. CONFIG & REPRODUCIBILITY\n",
    "# -----------------------------\n",
    "REPRODUCIBILITY_SEED = 42\n",
    "random.seed(REPRODUCIBILITY_SEED)\n",
    "np.random.seed(REPRODUCIBILITY_SEED)\n",
    "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optimized Hyperparameters for DCMVAE\n",
    "dim_x_features = 5  \n",
    "SEQUENCE_LENGTH = 30\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 128\n",
    "LATENT_DIM = 64  # Increased for better representation\n",
    "NUM_EPOCHS = 150  # More epochs for DCMVAE\n",
    "LEARNING_RATE = 5e-3\n",
    "KL_WEIGHT = 0.0005  # Reduced for stability\n",
    "MMD_WEIGHT = 100.0  # Increased for better balance\n",
    "ADJ_SPARSE_LAMBDA = 0.01  # Reduced for less aggressive sparsity\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1. UTILITIES\n",
    "# -----------------------------\n",
    "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
    "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
    "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
    "    cross = x @ y.t()\n",
    "    dists = x_norm + y_norm - 2 * cross\n",
    "    return torch.exp(-dists / (2 * (sigma ** 2) + 1e-12))\n",
    "\n",
    "def compute_mmd_stable(x, y, sigma=1.0):\n",
    "    if x is None or y is None: return torch.tensor(0.0, device=DEVICE)\n",
    "    n, m = x.size(0), y.size(0)\n",
    "    if n <= 1 or m <= 1: return torch.tensor(0.0, device=x.device)\n",
    "    K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
    "    K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
    "    K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
    "    sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
    "    sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
    "    sum_xy = K_xy.mean()\n",
    "    return sum_xx + sum_yy - 2.0 * sum_xy\n",
    "\n",
    "# -----------------------------\n",
    "# 2. DATA LOADER\n",
    "# -----------------------------\n",
    "class IHDP_TimeSeries:\n",
    "    def __init__(self, csv_path, batch_size, sequence_length, treatment_lag=9):\n",
    "        self.csv_path = csv_path\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.treatment_lag = treatment_lag\n",
    "        self._load_and_preprocess_data()\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_moving_window(series, window_size):\n",
    "        return pd.Series(series.flatten()).rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "    def _compute_lag(self, T, lag):\n",
    "        Tlag = np.zeros_like(T)\n",
    "        for i in range(lag, len(T)):\n",
    "            Tlag[i, 0] = T[i - lag, 0]\n",
    "        return Tlag\n",
    "\n",
    "    def _load_and_preprocess_data(self):\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            df = pd.DataFrame(np.random.randn(4000, 5), columns=['uoe', 'von', 'total_vel', 'zos', 'sithick'])\n",
    "        else:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
    "        y_base = df[['sithick']].values\n",
    "        ssh = df['zos'].values.reshape(-1, 1)\n",
    "        vel = df['total_vel'].values.reshape(-1, 1)\n",
    "\n",
    "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
    "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE).values.reshape(-1, 1)\n",
    "        T2_smooth = self.apply_moving_window(vel, WINDOW_SIZE).values.reshape(-1, 1)\n",
    "\n",
    "        T0_np = T0_smooth + (6.0 * T2_smooth) + (2.0 * hidden) + np.random.normal(0, 0.1, vel.shape)\n",
    "        v0 = np.mean(T2_smooth)\n",
    "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (T2_smooth - v0)))\n",
    "        T1_np = ((1.5 + 0.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
    "\n",
    "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
    "        T1_lag_np = self._compute_lag(T1_np, self.treatment_lag)\n",
    "        \n",
    "        X_FACTUAL = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
    "\n",
    "        num_seq = len(X_FACTUAL) // self.sequence_length\n",
    "        limit = num_seq * self.sequence_length\n",
    "\n",
    "        self.yall = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
    "        self.yall += np.random.normal(0, 0.2, self.yall.shape)\n",
    "\n",
    "        self.t0_raw = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
    "        self.t1_raw = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
    "        self.t0_lag = T0_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
    "        self.t1_lag = T1_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
    "        \n",
    "        self.x_scaler = StandardScaler()\n",
    "        self.xall = self.x_scaler.fit_transform(X_FACTUAL[:limit]).reshape(num_seq, self.sequence_length, dim_x_features)\n",
    "\n",
    "        self.y0_cf = self.yall\n",
    "        delta = -6.0 * np.abs(hidden[:limit].reshape(self.yall.shape)) * np.tanh(5.0 * (self.t1_raw - self.t0_raw))\n",
    "        self.y1_cf = self.yall + delta\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        vars = [self.xall, self.t0_raw, self.t1_raw, self.t0_lag, self.t1_lag, self.yall, self.y0_cf, self.y1_cf]\n",
    "        tr, te = zip(*[train_test_split(v, test_size=0.2, random_state=42) for v in vars])\n",
    "        def make_loader(data, shuffle):\n",
    "            ds = TensorDataset(*[torch.FloatTensor(v) for v in data])\n",
    "            return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle)\n",
    "        return make_loader(tr, True), make_loader(te, False)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ENHANCED DCMVAE MODEL\n",
    "# -----------------------------\n",
    "class DCMVAE(nn.Module):\n",
    "    def __init__(self, use_adj):\n",
    "        super().__init__()\n",
    "        self.use_adj = use_adj\n",
    "        \n",
    "        # Bidirectional GRU for better temporal modeling\n",
    "        self.encoder_rnn = nn.GRU(dim_x_features, HIDDEN_DIM, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Inference networks (increased capacity)\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, LATENT_DIM)\n",
    "        )\n",
    "        self.fc_logvar = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, LATENT_DIM)\n",
    "        )\n",
    "        \n",
    "        # Learnable adjacency matrix\n",
    "        self.adj_logits = nn.Parameter(0.1 * torch.randn(dim_x_features, dim_x_features))\n",
    "        \n",
    "        # Separate outcome heads with deeper networks\n",
    "        self.y0_head = nn.Sequential(\n",
    "            nn.Linear(LATENT_DIM, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.y1_head = nn.Sequential(\n",
    "            nn.Linear(LATENT_DIM, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, mode='train'):\n",
    "        # Apply causal graph structure\n",
    "        if self.use_adj:\n",
    "            mask = torch.sigmoid(self.adj_logits) * (1.0 - torch.eye(dim_x_features, device=DEVICE))\n",
    "            X = torch.matmul(X, mask)\n",
    "        else:\n",
    "            mask = torch.eye(dim_x_features, device=DEVICE)\n",
    "        \n",
    "        # Encode with bidirectional RNN\n",
    "        h_all, _ = self.encoder_rnn(X)\n",
    "        \n",
    "        # Variational inference\n",
    "        mu = self.fc_mu(h_all)\n",
    "        logvar = self.fc_logvar(h_all)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
    "        \n",
    "        # Predict outcomes\n",
    "        y0 = self.y0_head(z)\n",
    "        y1 = self.y1_head(z)\n",
    "        \n",
    "        return y0, y1, mu, logvar, z, mask\n",
    "\n",
    "    def counterfactual_prediction(self, X0, X1):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y0, _, _, _, _, _ = self.forward(X0, mode='eval')\n",
    "            _, y1, _, _, _, _ = self.forward(X1, mode='eval')\n",
    "        return y0.squeeze(-1), y1.squeeze(-1)\n",
    "\n",
    "    def factual_prediction(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y0, y1, _, _, _, _ = self.forward(X, mode='eval')\n",
    "            t_val = X[:, :, 3:4]\n",
    "            y_hat = torch.where(t_val > 0, y1, y0)\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. BASELINE MODELS\n",
    "# -----------------------------\n",
    "class R_CRN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
    "        self.y0_out = nn.Sequential(nn.Linear(HIDDEN_DIM, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        self.ite_out = nn.Sequential(nn.Linear(HIDDEN_DIM, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, x, t, t_lag):\n",
    "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
    "        y0_h = self.y0_out(z_seq)\n",
    "        ite_h = self.ite_out(z_seq)\n",
    "        return y0_h + t * ite_h, z_seq\n",
    "\n",
    "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
    "        z0, _ = self.rnn(torch.cat([x, t0_lag], dim=2))\n",
    "        y0_h = self.y0_out(z0)\n",
    "        ite_h = self.ite_out(z0)\n",
    "        return y0_h.squeeze(-1), (y0_h + ite_h).squeeze(-1)\n",
    "\n",
    "    def factual_prediction(self, x, t, t_lag):\n",
    "        y_h, _ = self.forward(x, t, t_lag)\n",
    "        return y_h.squeeze(-1)\n",
    "\n",
    "class CF_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
    "        self.y0_head = nn.Sequential(nn.Linear(HIDDEN_DIM, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        self.y1_head = nn.Sequential(nn.Linear(HIDDEN_DIM, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, x, t, t_lag):\n",
    "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
    "        y0_h, y1_h = self.y0_head(z_seq), self.y1_head(z_seq)\n",
    "        return (1 - t) * y0_h + t * y1_h, z_seq\n",
    "\n",
    "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
    "        z0, _ = self.rnn(torch.cat([x, t0_lag], dim=2))\n",
    "        return self.y0_head(z0).squeeze(-1), self.y1_head(z0).squeeze(-1)\n",
    "\n",
    "    def factual_prediction(self, x, t, t_lag):\n",
    "        y_h, _ = self.forward(x, t, t_lag)\n",
    "        return y_h.squeeze(-1)\n",
    "\n",
    "class TS_TARNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
    "        self.y0_head = nn.Sequential(nn.Linear(HIDDEN_DIM, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        self.y1_head = nn.Sequential(nn.Linear(HIDDEN_DIM, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, x, t, t_lag):\n",
    "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
    "        return (1 - t) * self.y0_head(z_seq) + t * self.y1_head(z_seq), z_seq\n",
    "\n",
    "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
    "        z0, _ = self.rnn(torch.cat([x, t0_lag], dim=2))\n",
    "        return self.y0_head(z0).squeeze(-1), self.y1_head(z0).squeeze(-1)\n",
    "\n",
    "    def factual_prediction(self, x, t, t_lag):\n",
    "        y_h, _ = self.forward(x, t, t_lag)\n",
    "        return y_h.squeeze(-1)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. OPTIMIZED TRAINING\n",
    "# -----------------------------\n",
    "def train_model(name, model, train_loader, device):\n",
    "    # DCMVAE gets special treatment\n",
    "    if name == 'DCMVAE':\n",
    "        lr = 3e-4\n",
    "        num_epochs = NUM_EPOCHS\n",
    "    else:\n",
    "        lr = LEARNING_RATE\n",
    "        num_epochs = 100\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=10)\n",
    "    \n",
    "    model.train()\n",
    "    t_median = 0.0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=name):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Progressive weighting for DCMVAE\n",
    "        if name == 'DCMVAE':\n",
    "            # Warm-up phases\n",
    "            kl_scale = min(1.0, epoch / 30.0)  # KL warm-up\n",
    "            mmd_scale = min(1.0, epoch / 50.0)  # MMD warm-up\n",
    "        \n",
    "        for x_in, t0, t1, t0_lag, t1_lag, y_out, y0_cf, y1_cf in train_loader:\n",
    "            opt.zero_grad()\n",
    "            x_in, y_f, t0, t0l = x_in.to(device), y_out.to(device), t0.to(device), t0_lag.to(device)\n",
    "            \n",
    "            if name == 'DCMVAE':\n",
    "                y0_p, y1_p, mu, logvar, z, mask = model(x_in)\n",
    "                \n",
    "                # Treatment assignment (use actual treatment from data)\n",
    "                t_idx = (t0 > t0.median()).float()\n",
    "                y_pred = (1 - t_idx) * y0_p + t_idx * y1_p\n",
    "                \n",
    "                # Multi-objective loss\n",
    "                loss_recon = F.mse_loss(y_pred, y_f)\n",
    "                loss_kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                \n",
    "                # Improved representation balancing\n",
    "                zf = z.reshape(-1, LATENT_DIM)\n",
    "                tf = t_idx.reshape(-1)\n",
    "                z0, z1 = zf[tf == 0], zf[tf == 1]\n",
    "                \n",
    "                if z0.size(0) > 1 and z1.size(0) > 1:\n",
    "                    # Multi-scale MMD for robustness\n",
    "                    loss_mmd = 0.0\n",
    "                    for sigma in [0.5, 1.0, 2.0]:\n",
    "                        loss_mmd += compute_mmd_stable(z0, z1, sigma=sigma)\n",
    "                    loss_mmd = loss_mmd / 3.0\n",
    "                else:\n",
    "                    loss_mmd = torch.tensor(0.0, device=device)\n",
    "                \n",
    "                # Adjacency sparsity\n",
    "                loss_adj = torch.norm(torch.sigmoid(model.adj_logits), 1)\n",
    "                \n",
    "                # Balanced combination with progressive scaling\n",
    "                loss = (10.0 * loss_recon + \n",
    "                       kl_scale * KL_WEIGHT * loss_kl + \n",
    "                       mmd_scale * MMD_WEIGHT * loss_mmd + \n",
    "                       ADJ_SPARSE_LAMBDA * loss_adj)\n",
    "                \n",
    "            else:\n",
    "                # Baseline models\n",
    "                y_p, z_s = model(x_in[:, :, :3], t0, t0l)\n",
    "                loss_recon = F.mse_loss(y_p, y_f)\n",
    "                loss_mmd = torch.tensor(0.0, device=device)\n",
    "                \n",
    "                if name != 'TS-TARNet':\n",
    "                    zf = z_s.reshape(-1, HIDDEN_DIM)\n",
    "                    tf = (t0 > t_median).reshape(-1)\n",
    "                    z0, z1 = zf[tf == 0], zf[tf == 1]\n",
    "                    if z0.size(0) > 1 and z1.size(0) > 1:\n",
    "                        loss_mmd = compute_mmd_stable(z0, z1) * MMD_WEIGHT\n",
    "                \n",
    "                loss = loss_recon + loss_mmd\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Learning rate scheduling for DCMVAE\n",
    "        if name == 'DCMVAE':\n",
    "            scheduler.step(epoch_loss / len(train_loader))\n",
    "\n",
    "# -----------------------------\n",
    "# 6. EVALUATION\n",
    "# -----------------------------\n",
    "def calculate_factual_rmse(model, loader, device, model_type):\n",
    "    model.eval()\n",
    "    se, count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
    "            yf = yf.to(device).squeeze(-1)\n",
    "            if model_type == 'DCMVAE':\n",
    "                yh = model.factual_prediction(x.to(device))\n",
    "            else:\n",
    "                yh = model.factual_prediction(x[:, :, :3].to(device), t0.to(device), t0l.to(device))\n",
    "            se += torch.sum((yh - yf) ** 2).item()\n",
    "            count += yf.numel()\n",
    "    return math.sqrt(se / count)\n",
    "\n",
    "def calculate_pehe_ate(model, loader, device, model_type):\n",
    "    model.eval()\n",
    "    ite_se, ate_err, count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
    "            y0t, y1t = y0c.to(device).squeeze(-1), y1c.to(device).squeeze(-1)\n",
    "            \n",
    "            if model_type == 'DCMVAE':\n",
    "                x0, x1 = x.clone().to(device), x.clone().to(device)\n",
    "                x0[:, :, 3], x0[:, :, 4] = t0.squeeze(-1), t0l.squeeze(-1)\n",
    "                x1[:, :, 3], x1[:, :, 4] = t1.squeeze(-1), t1l.squeeze(-1)\n",
    "                y0h, y1h = model.counterfactual_prediction(x0, x1)\n",
    "            else:\n",
    "                y0h, y1h = model.counterfactual_prediction(\n",
    "                    x[:, :, :3].to(device), t0.to(device), t1.to(device), \n",
    "                    t0l.to(device), t1l.to(device)\n",
    "                )\n",
    "            \n",
    "            ite_h, ite_t = (y1h - y0h), (y1t - y0t)\n",
    "            ite_se += torch.sum((ite_h - ite_t) ** 2).item()\n",
    "            ate_err += torch.sum(ite_h - ite_t).item()\n",
    "            count += y0t.numel()\n",
    "    \n",
    "    return math.sqrt(ite_se / count), abs(ate_err / count)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. BENCHMARK\n",
    "# -----------------------------\n",
    "def run_benchmark():\n",
    "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
    "    train_loader, test_loader = dm.get_dataloaders()\n",
    "    \n",
    "    model_list = {\n",
    "        'DCMVAE': DCMVAE(use_adj=True).to(DEVICE),\n",
    "        'R-CRN': R_CRN().to(DEVICE),\n",
    "        'CF-RNN': CF_RNN().to(DEVICE),\n",
    "        'TS-TARNet': TS_TARNet().to(DEVICE)\n",
    "    }\n",
    "    \n",
    "    for name, model in model_list.items():\n",
    "        train_model(name, model, train_loader, DEVICE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{'Model':<15} | {'Test RMSE':<12} | {'Test PEHE':<12} | {'Test ATE':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, model in model_list.items():\n",
    "        rmse = calculate_factual_rmse(model, test_loader, DEVICE, name)\n",
    "        pehe, ate = calculate_pehe_ate(model, test_loader, DEVICE, name)\n",
    "        print(f\"{name:<15} | {rmse:.4f}       | {pehe:.4f}       | {ate:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06230582-a373-421d-8f38-cc47364ec08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
