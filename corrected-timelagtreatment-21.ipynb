{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7767930d-806d-4534-939d-4af60f638ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training: MMD=True, ADJ=True\n",
      "============================================================\n",
      "✓ Test PEHE: 3.7939\n",
      "\n",
      "============================================================\n",
      "Training: MMD=False, ADJ=True\n",
      "============================================================\n",
      "✓ Test PEHE: 3.8581\n",
      "\n",
      "============================================================\n",
      "Training: MMD=True, ADJ=False\n",
      "============================================================\n",
      "✓ Test PEHE: 3.8760\n",
      "\n",
      "============================================================\n",
      "Training: MMD=False, ADJ=False\n",
      "============================================================\n",
      "✓ Test PEHE: 3.8667\n",
      "\n",
      "============================================================\n",
      "FINAL ABLATION RESULTS (Sequence-wide PEHE)\n",
      "============================================================\n",
      "MMD      | ADJ      | Test PEHE   \n",
      "------------------------------------------------------------\n",
      "✓        | ✓        | 3.7939\n",
      "✗        | ✓        | 3.8581\n",
      "✗        | ✗        | 3.8667\n",
      "✓        | ✗        | 3.8760\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -----------------------------\n",
    "# 0. CONFIG & REPRODUCIBILITY\n",
    "# -----------------------------\n",
    "REPRODUCIBILITY_SEED = 42\n",
    "random.seed(REPRODUCIBILITY_SEED)\n",
    "np.random.seed(REPRODUCIBILITY_SEED)\n",
    "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optimized Hyperparameters\n",
    "dim_x_features = 5  \n",
    "SEQUENCE_LENGTH = 30\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 128\n",
    "LATENT_DIM = 64  # Increased for better representation\n",
    "NUM_EPOCHS = 150  # More training for complex model\n",
    "LEARNING_RATE = 5e-4  # Lower for stability\n",
    "KL_WEIGHT = 0.0005  # Reduced for stability\n",
    "MMD_WEIGHT = 100.0  # Increased for better balance\n",
    "ADJ_SPARSE_LAMBDA = 0.01  # Reduced for flexibility\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1. UTILITIES\n",
    "# -----------------------------\n",
    "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
    "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
    "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
    "    cross = x @ y.t()\n",
    "    dists = x_norm + y_norm - 2 * cross\n",
    "    return torch.exp(-dists / (2 * (sigma ** 2) + 1e-12))\n",
    "\n",
    "def compute_mmd_stable(x, y, sigma=1.0):\n",
    "    if x is None or y is None: return torch.tensor(0.0, device=DEVICE)\n",
    "    n, m = x.size(0), y.size(0)\n",
    "    if n <= 1 or m <= 1: return torch.tensor(0.0, device=x.device)\n",
    "    K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
    "    K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
    "    K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
    "    sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
    "    sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
    "    sum_xy = K_xy.mean()\n",
    "    return sum_xx + sum_yy - 2.0 * sum_xy\n",
    "\n",
    "# -----------------------------\n",
    "# 2. DATA LOADER\n",
    "# -----------------------------\n",
    "class IHDP_TimeSeries:\n",
    "    def __init__(self, csv_path, batch_size, sequence_length, treatment_lag=1):\n",
    "        self.csv_path = csv_path\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.treatment_lag = treatment_lag\n",
    "        self._load_and_preprocess_data()\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_moving_window(series, window_size):\n",
    "        return pd.Series(series.flatten()).rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "    def _compute_lag(self, T, lag):\n",
    "        Tlag = np.zeros_like(T)\n",
    "        for i in range(lag, len(T)):\n",
    "            Tlag[i, 0] = T[i - lag, 0]\n",
    "        return Tlag\n",
    "\n",
    "    def _load_and_preprocess_data(self):\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            df = pd.DataFrame(np.random.randn(4000, 5), columns=['uoe', 'von', 'total_vel', 'zos', 'sithick'])\n",
    "        else:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
    "        y_base = df[['sithick']].values\n",
    "        ssh = df['zos'].values.reshape(-1, 1)\n",
    "        vel = df['total_vel'].values.reshape(-1, 1)\n",
    "\n",
    "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
    "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE).values.reshape(-1, 1)\n",
    "        T2_smooth = self.apply_moving_window(vel, WINDOW_SIZE).values.reshape(-1, 1)\n",
    "\n",
    "        T0_np = T0_smooth + (6.0 * T2_smooth) + (2.0 * hidden) + np.random.normal(0, 0.1, vel.shape)\n",
    "        v0 = np.mean(T2_smooth)\n",
    "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (T2_smooth - v0)))\n",
    "        T1_np = ((1.5 + 0.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
    "\n",
    "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
    "        X_FACTUAL = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
    "\n",
    "        num_seq = len(X_FACTUAL) // self.sequence_length\n",
    "        limit = num_seq * self.sequence_length\n",
    "\n",
    "        self.yall = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
    "        self.yall += np.random.normal(0, 0.2, self.yall.shape)\n",
    "\n",
    "        self.t_raw = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
    "        self.x_scaler = StandardScaler()\n",
    "        self.xall = self.x_scaler.fit_transform(X_FACTUAL[:limit]).reshape(num_seq, self.sequence_length, dim_x_features)\n",
    "\n",
    "        self.y0_cf = self.yall\n",
    "        delta = -6.0 * np.abs(hidden[:limit].reshape(self.yall.shape)) * np.tanh(5.0 * (T1_np[:limit] - T0_np[:limit]).reshape(self.yall.shape))\n",
    "        self.y1_cf = self.yall + delta\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        vars = [self.xall, self.t_raw, self.yall, self.y0_cf, self.y1_cf]\n",
    "        tr, te = zip(*[train_test_split(v, test_size=0.2, random_state=42) for v in vars])\n",
    "        def make_loader(data, shuffle):\n",
    "            ds = TensorDataset(*[torch.FloatTensor(v) for v in data])\n",
    "            return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle)\n",
    "        return make_loader(tr, True), make_loader(te, False)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ENHANCED DCMVAE MODEL\n",
    "# -----------------------------\n",
    "class DCMVAE(nn.Module):\n",
    "    def __init__(self, use_adj):\n",
    "        super().__init__()\n",
    "        self.use_adj = use_adj\n",
    "        \n",
    "        # Bidirectional GRU for better temporal modeling\n",
    "        self.encoder_rnn = nn.GRU(dim_x_features, HIDDEN_DIM, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Enhanced inference networks\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(HIDDEN_DIM, LATENT_DIM)\n",
    "        )\n",
    "        self.fc_logvar = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(HIDDEN_DIM, LATENT_DIM)\n",
    "        )\n",
    "        \n",
    "        # Learnable causal adjacency matrix\n",
    "        self.adj_logits = nn.Parameter(0.1 * torch.randn(dim_x_features, dim_x_features))\n",
    "        \n",
    "        # Deeper outcome heads with residual-like connections\n",
    "        self.y0_head = nn.Sequential(\n",
    "            nn.Linear(LATENT_DIM, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.y1_head = nn.Sequential(\n",
    "            nn.Linear(LATENT_DIM, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, mode='train'):\n",
    "        # Apply causal graph structure\n",
    "        if self.use_adj:\n",
    "            mask = torch.sigmoid(self.adj_logits) * (1.0 - torch.eye(dim_x_features, device=DEVICE))\n",
    "            X = torch.matmul(X, mask)\n",
    "        else:\n",
    "            mask = torch.eye(dim_x_features, device=DEVICE)\n",
    "\n",
    "        # Encode with bidirectional RNN\n",
    "        h_all, _ = self.encoder_rnn(X)\n",
    "        \n",
    "        # Variational inference\n",
    "        mu = self.fc_mu(h_all)\n",
    "        logvar = self.fc_logvar(h_all)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
    "        \n",
    "        # Predict potential outcomes\n",
    "        y0 = self.y0_head(z)\n",
    "        y1 = self.y1_head(z)\n",
    "        \n",
    "        return y0, y1, mu, logvar, z, mask\n",
    "\n",
    "    def counterfactual_prediction(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y0, y1, _, _, _, _ = self.forward(X, mode='eval')\n",
    "        return y0, y1\n",
    "\n",
    "# -----------------------------\n",
    "# 4. OPTIMIZED TRAINING\n",
    "# -----------------------------\n",
    "def train_model(model, train_loader, use_mmd, use_adj, t_median):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Progressive warm-up for regularization terms\n",
    "        kl_scale = min(1.0, epoch / 30.0)  # KL warm-up over 30 epochs\n",
    "        mmd_scale = min(1.0, epoch / 50.0)  # MMD warm-up over 50 epochs\n",
    "        \n",
    "        for x_in, t_raw, y_f, y0_cf, y1_cf in train_loader:\n",
    "            x_in, y_f, t_raw = x_in.to(DEVICE), y_f.to(DEVICE), t_raw.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y0, y1, mu, logvar, z, mask = model(x_in, mode='train')\n",
    "            t_idx = (t_raw > t_median).float()\n",
    "\n",
    "            # Factual prediction with treatment assignment\n",
    "            y_pred = (1 - t_idx) * y0 + t_idx * y1\n",
    "            \n",
    "            # Reconstruction loss (prioritized)\n",
    "            loss_recon = F.mse_loss(y_pred, y_f)\n",
    "            \n",
    "            # KL divergence with warm-up\n",
    "            loss_kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            \n",
    "            # MMD loss for representation balancing\n",
    "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
    "            if use_mmd:\n",
    "                z_flat = z.view(-1, LATENT_DIM)\n",
    "                t_flat = t_idx.view(-1)\n",
    "                z0, z1 = z_flat[t_flat == 0], z_flat[t_flat == 1]\n",
    "                \n",
    "                if z0.size(0) > 1 and z1.size(0) > 1:\n",
    "                    # Multi-scale MMD for robustness\n",
    "                    mmd_sum = 0.0\n",
    "                    for sigma in [0.5, 1.0, 2.0]:\n",
    "                        mmd_sum += compute_mmd_stable(z0, z1, sigma=sigma)\n",
    "                    loss_mmd = (mmd_sum / 3.0) * mmd_scale\n",
    "\n",
    "            # Adjacency sparsity regularization\n",
    "            loss_sparse = torch.tensor(0.0, device=DEVICE)\n",
    "            if use_adj:\n",
    "                loss_sparse = torch.norm(mask, 1)\n",
    "\n",
    "            # Combined loss with careful weighting\n",
    "            loss = (10.0 * loss_recon + \n",
    "                   kl_scale * KL_WEIGHT * loss_kl + \n",
    "                   MMD_WEIGHT * loss_mmd + \n",
    "                   ADJ_SPARSE_LAMBDA * loss_sparse)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(epoch_loss / len(train_loader))\n",
    "\n",
    "# -----------------------------\n",
    "# 5. ABLATION BENCHMARK\n",
    "# -----------------------------\n",
    "def run_benchmark():\n",
    "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
    "    train_loader, test_loader = dm.get_dataloaders()\n",
    "    t_median = np.median(dm.t_raw)\n",
    "\n",
    "    # Ablation configurations\n",
    "    configs = [\n",
    "        (True, True),   # Full model: MMD + ADJ\n",
    "        (False, True),  # No MMD, only ADJ\n",
    "        (True, False),  # No ADJ, only MMD\n",
    "        (False, False)  # Baseline: no MMD, no ADJ\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for use_mmd, use_adj in configs:\n",
    "        config_name = f\"MMD={use_mmd}, ADJ={use_adj}\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {config_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model = DCMVAE(use_adj=use_adj).to(DEVICE)\n",
    "        train_model(model, train_loader, use_mmd, use_adj, t_median)\n",
    "\n",
    "        # Evaluation: Sequence-wide PEHE\n",
    "        model.eval()\n",
    "        pehe_sum, count = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_in, t_raw, y_f, y0_cf, y1_cf in test_loader:\n",
    "                p0, p1 = model.counterfactual_prediction(x_in.to(DEVICE))\n",
    "                \n",
    "                # Individual Treatment Effect\n",
    "                ite_pred = p1 - p0\n",
    "                ite_true = (y1_cf - y0_cf).to(DEVICE)\n",
    "                \n",
    "                # PEHE: sqrt(mean((ITE_pred - ITE_true)^2))\n",
    "                pehe_sum += torch.sum((ite_pred - ite_true) ** 2).item()\n",
    "                count += x_in.size(0) * SEQUENCE_LENGTH\n",
    "\n",
    "        pehe = math.sqrt(pehe_sum / count)\n",
    "        results.append({\"MMD\": use_mmd, \"ADJ\": use_adj, \"PEHE\": pehe})\n",
    "        print(f\"✓ Test PEHE: {pehe:.4f}\")\n",
    "\n",
    "    # Display results table\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL ABLATION RESULTS (Sequence-wide PEHE)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'MMD':<8} | {'ADJ':<8} | {'Test PEHE':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for r in sorted(results, key=lambda x: x['PEHE']):\n",
    "        mmd_str = \"✓\" if r['MMD'] else \"✗\"\n",
    "        adj_str = \"✓\" if r['ADJ'] else \"✗\"\n",
    "        print(f\"{mmd_str:<8} | {adj_str:<8} | {r['PEHE']:.4f}\")\n",
    "    \n",
    "    # Highlight best configuration\n",
    "    best = min(results, key=lambda x: x['PEHE'])\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24673675-832b-4c2b-8679-67b0f6c7f6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
