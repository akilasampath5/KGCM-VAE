{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc0mKiQjdMh8",
        "outputId": "8dead788-51c2-45a9-ff31-3f725aaaf201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 1\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path         = csv_path\n",
        "        self.batch_size       = batch_size\n",
        "        self.sequence_length  = sequence_length\n",
        "        self.treatment_lag    = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # --- Control treatment T0 ---\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # --- Treated treatment T1: independent additive component ---\n",
        "        # Fix: T1 is NOT 1.5*T0 (that gives correlation=1.0)\n",
        "        # Adding 2*hidden + noise breaks the perfect correlation\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        # --- Regime-Dependent Treated treatment T1 ---\n",
        "        v0 = np.mean(vel) # Using the raw velocity baseline\n",
        "        # High velocity results in a sigmoid approaching 1.0, scaling T0 by 2.0x\n",
        "        # Low velocity results in a sigmoid approaching 0.0, scaling T0 by 1.5x\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1) # Range expanded\n",
        "\n",
        "        # T1 is now a non-linear scaling of T0 based on current ice velocity\n",
        "        #T1_np = ((1.5 + 0.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "        #T1_np = T0_np + 2.0 * hidden + np.random.normal(0, 0.5, T0_np.shape)\n",
        "\n",
        "        # --- Treatment lag (Fix: was duplicate T0 before) ---\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        # Scale and reshape\n",
        "        scaler        = StandardScaler()\n",
        "        X_scaled      = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # --- Counterfactual outcomes ---\n",
        "        # Fix: delta now depends on (T1 - T0) so treatment has a real causal effect\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}  (target: < 1.0)\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices          = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx   = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include T1, Y0, Y1 for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        # Preserves T0/T1 values unchanged\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        # Concatenate treatment with covariates before encoding\n",
        "        x_and_t = torch.cat([X, t], dim=-1)         # (B, T, dim_x+1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)          # (B, T, HIDDEN*2)\n",
        "\n",
        "        mu     = self.fc_mu(h)                        # (B, T, LATENT)\n",
        "        logvar = self.fc_logvar(h)                    # (B, T, LATENT)\n",
        "\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        z   = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "\n",
        "        # Project treatment and concatenate with latent\n",
        "        t_emb  = self.t_proj(t)                       # (B, T, 16)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)       # (B, T, LATENT+16)\n",
        "        y_pred  = self.outcome_head(z_and_t)           # (B, T, 1)\n",
        "\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "# ============================================================\n",
        "# 4. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_model(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)   # KL warmup\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)   # MMD warmup\n",
        "\n",
        "        for batch_idx, (x_in, t_fact, y_fact) in enumerate(train_loader):\n",
        "            x_in, t_fact, y_fact = (\n",
        "                x_in.to(DEVICE), t_fact.to(DEVICE), y_fact.to(DEVICE)\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t_fact, mode='train')\n",
        "\n",
        "            # Reconstruction: factual outcome only\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "\n",
        "            # KL with warmup and clamp to prevent explosion\n",
        "            loss_kl = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            # MMD: balance treated vs control latent representations\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t_fact.view(-1) > t_median).float()\n",
        "                z0     = z_flat[t_flat == 0]\n",
        "                z1     = z_flat[t_flat == 1]\n",
        "\n",
        "                # Print group sizes once per epoch for diagnosis\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0.size(0)}, z1={z1.size(0)}\")\n",
        "\n",
        "                loss_mmd = compute_mmd_stable(z0, z1) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    pehe_sum = 0.0\n",
        "    count    = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x_in, t_f, t_c, y0_gt, y1_gt) in enumerate(test_loader):\n",
        "            x_in = x_in.to(DEVICE)\n",
        "            t_f  = t_f.to(DEVICE)   # T0: control treatment\n",
        "            t_c  = t_c.to(DEVICE)   # T1: treated treatment\n",
        "\n",
        "            # Predict outcome under T0 and T1\n",
        "            p0, _, _, _ = model(x_in, t_f, mode='eval')\n",
        "            p1, _, _, _ = model(x_in, t_c, mode='eval')\n",
        "\n",
        "            ite_pred = p1 - p0\n",
        "            ite_true = (y1_gt - y0_gt).to(DEVICE)\n",
        "\n",
        "            # ITE diagnostic on first batch\n",
        "            if batch_idx == 0:\n",
        "                print(f\"  p0 mean={p0.mean().item():.4f}  \"\n",
        "                      f\"p1 mean={p1.mean().item():.4f}\")\n",
        "                print(f\"  Pred ITE mean={ite_pred.mean().item():.4f}  \"\n",
        "                      f\"True ITE mean={ite_true.mean().item():.4f}\")\n",
        "\n",
        "            pehe_sum += torch.sum((ite_pred - ite_true) ** 2).item()\n",
        "            count    += x_in.size(0) * SEQUENCE_LENGTH\n",
        "\n",
        "    return math.sqrt(pehe_sum / count)\n",
        "\n",
        "# ============================================================\n",
        "# 6. ABLATION BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    # Full ablation: MMD on/off\n",
        "    configs = [True, False]\n",
        "    results = []\n",
        "\n",
        "    for use_mmd in configs:\n",
        "        label = \"MMD=True \" if use_mmd else \"MMD=False\"\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {label}\")\n",
        "        print(f\"{'='*55}\")\n",
        "\n",
        "        model = DCMVAE(use_mmd=use_mmd).to(DEVICE)\n",
        "        train_model(model, train_loader, t_median)\n",
        "\n",
        "        print(f\"\\nEvaluating {label}...\")\n",
        "        pehe = evaluate(model, test_loader)\n",
        "        print(f\"PEHE: {pehe:.4f}\")\n",
        "        results.append((label, pehe))\n",
        "\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(\"FINAL ABLATION RESULTS\")\n",
        "    print(f\"{'='*55}\")\n",
        "    print(f\"{'Config':<12} | {'Test PEHE'}\")\n",
        "    print(\"-\" * 35)\n",
        "    for label, pehe in sorted(results, key=lambda x: x[1]):\n",
        "        marker = \" ← best\" if pehe == min(r[1] for r in results) else \"\"\n",
        "        print(f\"{label:<12} | {pehe:.4f}{marker}\")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei90qXhCmKke",
        "outputId": "12ae90fd-c8f4-4a4c-dbb3-ff2574c10b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443  (target: < 1.0)\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: MMD=True \n",
            "=======================================================\n",
            "  [MMD] z0=418, z1=542\n",
            "Epoch 000 | loss=10.1360 | recon=1.0490 kl=0.0055 mmd=0.000000\n",
            "  [MMD] z0=439, z1=521\n",
            "Epoch 030 | loss=9.7046 | recon=0.9419 kl=0.0377 mmd=0.000001\n",
            "  [MMD] z0=453, z1=507\n",
            "Epoch 060 | loss=9.8188 | recon=0.9927 kl=0.1189 mmd=0.000001\n",
            "  [MMD] z0=501, z1=459\n",
            "Epoch 090 | loss=10.2375 | recon=1.1260 kl=0.1378 mmd=0.000002\n",
            "  [MMD] z0=482, z1=478\n",
            "Epoch 120 | loss=9.9621 | recon=1.0302 kl=0.1528 mmd=0.000002\n",
            "  [MMD] z0=509, z1=451\n",
            "Epoch 149 | loss=9.8733 | recon=1.0181 kl=0.1580 mmd=0.000001\n",
            "\n",
            "Evaluating MMD=True ...\n",
            "  p0 mean=0.0071  p1 mean=0.0075\n",
            "  Pred ITE mean=0.0004  True ITE mean=0.6843\n",
            "PEHE: 3.0883\n",
            "\n",
            "=======================================================\n",
            "Training: MMD=False\n",
            "=======================================================\n",
            "Epoch 000 | loss=10.0251 | recon=1.0152 kl=0.0056 mmd=0.000000\n",
            "Epoch 030 | loss=10.0002 | recon=1.0277 kl=0.0150 mmd=0.000000\n",
            "Epoch 060 | loss=9.8208 | recon=0.9778 kl=0.0198 mmd=0.000000\n",
            "Epoch 090 | loss=10.0617 | recon=1.0498 kl=0.0226 mmd=0.000000\n",
            "Epoch 120 | loss=9.7085 | recon=0.9598 kl=0.0255 mmd=0.000000\n",
            "Epoch 149 | loss=9.9765 | recon=1.0344 kl=0.0204 mmd=0.000000\n",
            "\n",
            "Evaluating MMD=False...\n",
            "  p0 mean=-0.0592  p1 mean=-0.0508\n",
            "  Pred ITE mean=0.0084  True ITE mean=0.6843\n",
            "PEHE: 3.1201\n",
            "\n",
            "=======================================================\n",
            "FINAL ABLATION RESULTS\n",
            "=======================================================\n",
            "Config       | Test PEHE\n",
            "-----------------------------------\n",
            "MMD=True     | 3.0883 ← best\n",
            "MMD=False    | 3.1201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 3\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path         = csv_path\n",
        "        self.batch_size       = batch_size\n",
        "        self.sequence_length  = sequence_length\n",
        "        self.treatment_lag    = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # --- Control treatment T0 ---\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # --- Treated treatment T1: independent additive component ---\n",
        "        # Fix: T1 is NOT 1.5*T0 (that gives correlation=1.0)\n",
        "        # Adding 2*hidden + noise breaks the perfect correlation\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        # --- Regime-Dependent Treated treatment T1 ---\n",
        "        v0 = np.mean(vel) # Using the raw velocity baseline\n",
        "        # High velocity results in a sigmoid approaching 1.0, scaling T0 by 2.0x\n",
        "        # Low velocity results in a sigmoid approaching 0.0, scaling T0 by 1.5x\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1) # Range expanded\n",
        "\n",
        "        # T1 is now a non-linear scaling of T0 based on current ice velocity\n",
        "        #T1_np = ((1.5 + 0.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "        #T1_np = T0_np + 2.0 * hidden + np.random.normal(0, 0.5, T0_np.shape)\n",
        "\n",
        "        # --- Treatment lag (Fix: was duplicate T0 before) ---\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        # Scale and reshape\n",
        "        scaler        = StandardScaler()\n",
        "        X_scaled      = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # --- Counterfactual outcomes ---\n",
        "        # Fix: delta now depends on (T1 - T0) so treatment has a real causal effect\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}  (target: < 1.0)\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices          = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx   = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include T1, Y0, Y1 for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        # Preserves T0/T1 values unchanged\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        # Concatenate treatment with covariates before encoding\n",
        "        x_and_t = torch.cat([X, t], dim=-1)         # (B, T, dim_x+1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)          # (B, T, HIDDEN*2)\n",
        "\n",
        "        mu     = self.fc_mu(h)                        # (B, T, LATENT)\n",
        "        logvar = self.fc_logvar(h)                    # (B, T, LATENT)\n",
        "\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        z   = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "\n",
        "        # Project treatment and concatenate with latent\n",
        "        t_emb  = self.t_proj(t)                       # (B, T, 16)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)       # (B, T, LATENT+16)\n",
        "        y_pred  = self.outcome_head(z_and_t)           # (B, T, 1)\n",
        "\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "# ============================================================\n",
        "# 4. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_model(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)   # KL warmup\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)   # MMD warmup\n",
        "\n",
        "        for batch_idx, (x_in, t_fact, y_fact) in enumerate(train_loader):\n",
        "            x_in, t_fact, y_fact = (\n",
        "                x_in.to(DEVICE), t_fact.to(DEVICE), y_fact.to(DEVICE)\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t_fact, mode='train')\n",
        "\n",
        "            # Reconstruction: factual outcome only\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "\n",
        "            # KL with warmup and clamp to prevent explosion\n",
        "            loss_kl = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            # MMD: balance treated vs control latent representations\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t_fact.view(-1) > t_median).float()\n",
        "                z0     = z_flat[t_flat == 0]\n",
        "                z1     = z_flat[t_flat == 1]\n",
        "\n",
        "                # Print group sizes once per epoch for diagnosis\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0.size(0)}, z1={z1.size(0)}\")\n",
        "\n",
        "                loss_mmd = compute_mmd_stable(z0, z1) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    pehe_sum = 0.0\n",
        "    count    = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x_in, t_f, t_c, y0_gt, y1_gt) in enumerate(test_loader):\n",
        "            x_in = x_in.to(DEVICE)\n",
        "            t_f  = t_f.to(DEVICE)   # T0: control treatment\n",
        "            t_c  = t_c.to(DEVICE)   # T1: treated treatment\n",
        "\n",
        "            # Predict outcome under T0 and T1\n",
        "            p0, _, _, _ = model(x_in, t_f, mode='eval')\n",
        "            p1, _, _, _ = model(x_in, t_c, mode='eval')\n",
        "\n",
        "            ite_pred = p1 - p0\n",
        "            ite_true = (y1_gt - y0_gt).to(DEVICE)\n",
        "\n",
        "            # ITE diagnostic on first batch\n",
        "            if batch_idx == 0:\n",
        "                print(f\"  p0 mean={p0.mean().item():.4f}  \"\n",
        "                      f\"p1 mean={p1.mean().item():.4f}\")\n",
        "                print(f\"  Pred ITE mean={ite_pred.mean().item():.4f}  \"\n",
        "                      f\"True ITE mean={ite_true.mean().item():.4f}\")\n",
        "\n",
        "            pehe_sum += torch.sum((ite_pred - ite_true) ** 2).item()\n",
        "            count    += x_in.size(0) * SEQUENCE_LENGTH\n",
        "\n",
        "    return math.sqrt(pehe_sum / count)\n",
        "\n",
        "# ============================================================\n",
        "# 6. ABLATION BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    # Full ablation: MMD on/off\n",
        "    configs = [True, False]\n",
        "    results = []\n",
        "\n",
        "    for use_mmd in configs:\n",
        "        label = \"MMD=True \" if use_mmd else \"MMD=False\"\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {label}\")\n",
        "        print(f\"{'='*55}\")\n",
        "\n",
        "        model = DCMVAE(use_mmd=use_mmd).to(DEVICE)\n",
        "        train_model(model, train_loader, t_median)\n",
        "\n",
        "        print(f\"\\nEvaluating {label}...\")\n",
        "        pehe = evaluate(model, test_loader)\n",
        "        print(f\"PEHE: {pehe:.4f}\")\n",
        "        results.append((label, pehe))\n",
        "\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(\"FINAL ABLATION RESULTS\")\n",
        "    print(f\"{'='*55}\")\n",
        "    print(f\"{'Config':<12} | {'Test PEHE'}\")\n",
        "    print(\"-\" * 35)\n",
        "    for label, pehe in sorted(results, key=lambda x: x[1]):\n",
        "        marker = \" ← best\" if pehe == min(r[1] for r in results) else \"\"\n",
        "        print(f\"{label:<12} | {pehe:.4f}{marker}\")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DplDjfQu2o8",
        "outputId": "50356bd0-5c64-4e71-8b7b-b06ccb707791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443  (target: < 1.0)\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: MMD=True \n",
            "=======================================================\n",
            "  [MMD] z0=418, z1=542\n",
            "Epoch 000 | loss=10.1363 | recon=1.0491 kl=0.0055 mmd=0.000000\n",
            "  [MMD] z0=439, z1=521\n",
            "Epoch 030 | loss=9.7016 | recon=0.9416 kl=0.0388 mmd=0.000001\n",
            "  [MMD] z0=453, z1=507\n",
            "Epoch 060 | loss=9.8095 | recon=0.9924 kl=0.1246 mmd=0.000001\n",
            "  [MMD] z0=501, z1=459\n",
            "Epoch 090 | loss=10.2231 | recon=1.1245 kl=0.1293 mmd=0.000002\n",
            "  [MMD] z0=482, z1=478\n",
            "Epoch 120 | loss=9.9424 | recon=1.0275 kl=0.1523 mmd=0.000001\n",
            "  [MMD] z0=509, z1=451\n",
            "Epoch 149 | loss=9.8597 | recon=1.0165 kl=0.1527 mmd=0.000001\n",
            "\n",
            "Evaluating MMD=True ...\n",
            "  p0 mean=0.0081  p1 mean=0.0080\n",
            "  Pred ITE mean=-0.0001  True ITE mean=0.6843\n",
            "PEHE: 3.0810\n",
            "\n",
            "=======================================================\n",
            "Training: MMD=False\n",
            "=======================================================\n",
            "Epoch 000 | loss=10.0249 | recon=1.0152 kl=0.0055 mmd=0.000000\n",
            "Epoch 030 | loss=9.9999 | recon=1.0276 kl=0.0159 mmd=0.000000\n",
            "Epoch 060 | loss=9.8171 | recon=0.9770 kl=0.0220 mmd=0.000000\n",
            "Epoch 090 | loss=10.0557 | recon=1.0489 kl=0.0252 mmd=0.000000\n",
            "Epoch 120 | loss=9.7025 | recon=0.9592 kl=0.0282 mmd=0.000000\n",
            "Epoch 149 | loss=9.9688 | recon=1.0329 kl=0.0233 mmd=0.000000\n",
            "\n",
            "Evaluating MMD=False...\n",
            "  p0 mean=-0.0582  p1 mean=-0.0510\n",
            "  Pred ITE mean=0.0072  True ITE mean=0.6843\n",
            "PEHE: 3.1160\n",
            "\n",
            "=======================================================\n",
            "FINAL ABLATION RESULTS\n",
            "=======================================================\n",
            "Config       | Test PEHE\n",
            "-----------------------------------\n",
            "MMD=True     | 3.0810 ← best\n",
            "MMD=False    | 3.1160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 6\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path         = csv_path\n",
        "        self.batch_size       = batch_size\n",
        "        self.sequence_length  = sequence_length\n",
        "        self.treatment_lag    = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # --- Control treatment T0 ---\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # --- Treated treatment T1: independent additive component ---\n",
        "        # Fix: T1 is NOT 1.5*T0 (that gives correlation=1.0)\n",
        "        # Adding 2*hidden + noise breaks the perfect correlation\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        # --- Regime-Dependent Treated treatment T1 ---\n",
        "        v0 = np.mean(vel) # Using the raw velocity baseline\n",
        "        # High velocity results in a sigmoid approaching 1.0, scaling T0 by 2.0x\n",
        "        # Low velocity results in a sigmoid approaching 0.0, scaling T0 by 1.5x\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1) # Range expanded\n",
        "\n",
        "        # T1 is now a non-linear scaling of T0 based on current ice velocity\n",
        "        #T1_np = ((1.5 + 0.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "        #T1_np = T0_np + 2.0 * hidden + np.random.normal(0, 0.5, T0_np.shape)\n",
        "\n",
        "        # --- Treatment lag (Fix: was duplicate T0 before) ---\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        # Scale and reshape\n",
        "        scaler        = StandardScaler()\n",
        "        X_scaled      = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # --- Counterfactual outcomes ---\n",
        "        # Fix: delta now depends on (T1 - T0) so treatment has a real causal effect\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}  (target: < 1.0)\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices          = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx   = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include T1, Y0, Y1 for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        # Preserves T0/T1 values unchanged\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        # Concatenate treatment with covariates before encoding\n",
        "        x_and_t = torch.cat([X, t], dim=-1)         # (B, T, dim_x+1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)          # (B, T, HIDDEN*2)\n",
        "\n",
        "        mu     = self.fc_mu(h)                        # (B, T, LATENT)\n",
        "        logvar = self.fc_logvar(h)                    # (B, T, LATENT)\n",
        "\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        z   = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "\n",
        "        # Project treatment and concatenate with latent\n",
        "        t_emb  = self.t_proj(t)                       # (B, T, 16)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)       # (B, T, LATENT+16)\n",
        "        y_pred  = self.outcome_head(z_and_t)           # (B, T, 1)\n",
        "\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "# ============================================================\n",
        "# 4. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_model(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)   # KL warmup\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)   # MMD warmup\n",
        "\n",
        "        for batch_idx, (x_in, t_fact, y_fact) in enumerate(train_loader):\n",
        "            x_in, t_fact, y_fact = (\n",
        "                x_in.to(DEVICE), t_fact.to(DEVICE), y_fact.to(DEVICE)\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t_fact, mode='train')\n",
        "\n",
        "            # Reconstruction: factual outcome only\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "\n",
        "            # KL with warmup and clamp to prevent explosion\n",
        "            loss_kl = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            # MMD: balance treated vs control latent representations\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t_fact.view(-1) > t_median).float()\n",
        "                z0     = z_flat[t_flat == 0]\n",
        "                z1     = z_flat[t_flat == 1]\n",
        "\n",
        "                # Print group sizes once per epoch for diagnosis\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0.size(0)}, z1={z1.size(0)}\")\n",
        "\n",
        "                loss_mmd = compute_mmd_stable(z0, z1) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    pehe_sum = 0.0\n",
        "    count    = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x_in, t_f, t_c, y0_gt, y1_gt) in enumerate(test_loader):\n",
        "            x_in = x_in.to(DEVICE)\n",
        "            t_f  = t_f.to(DEVICE)   # T0: control treatment\n",
        "            t_c  = t_c.to(DEVICE)   # T1: treated treatment\n",
        "\n",
        "            # Predict outcome under T0 and T1\n",
        "            p0, _, _, _ = model(x_in, t_f, mode='eval')\n",
        "            p1, _, _, _ = model(x_in, t_c, mode='eval')\n",
        "\n",
        "            ite_pred = p1 - p0\n",
        "            ite_true = (y1_gt - y0_gt).to(DEVICE)\n",
        "\n",
        "            # ITE diagnostic on first batch\n",
        "            if batch_idx == 0:\n",
        "                print(f\"  p0 mean={p0.mean().item():.4f}  \"\n",
        "                      f\"p1 mean={p1.mean().item():.4f}\")\n",
        "                print(f\"  Pred ITE mean={ite_pred.mean().item():.4f}  \"\n",
        "                      f\"True ITE mean={ite_true.mean().item():.4f}\")\n",
        "\n",
        "            pehe_sum += torch.sum((ite_pred - ite_true) ** 2).item()\n",
        "            count    += x_in.size(0) * SEQUENCE_LENGTH\n",
        "\n",
        "    return math.sqrt(pehe_sum / count)\n",
        "\n",
        "# ============================================================\n",
        "# 6. ABLATION BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    # Full ablation: MMD on/off\n",
        "    configs = [True, False]\n",
        "    results = []\n",
        "\n",
        "    for use_mmd in configs:\n",
        "        label = \"MMD=True \" if use_mmd else \"MMD=False\"\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {label}\")\n",
        "        print(f\"{'='*55}\")\n",
        "\n",
        "        model = DCMVAE(use_mmd=use_mmd).to(DEVICE)\n",
        "        train_model(model, train_loader, t_median)\n",
        "\n",
        "        print(f\"\\nEvaluating {label}...\")\n",
        "        pehe = evaluate(model, test_loader)\n",
        "        print(f\"PEHE: {pehe:.4f}\")\n",
        "        results.append((label, pehe))\n",
        "\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(\"FINAL ABLATION RESULTS\")\n",
        "    print(f\"{'='*55}\")\n",
        "    print(f\"{'Config':<12} | {'Test PEHE'}\")\n",
        "    print(\"-\" * 35)\n",
        "    for label, pehe in sorted(results, key=lambda x: x[1]):\n",
        "        marker = \" ← best\" if pehe == min(r[1] for r in results) else \"\"\n",
        "        print(f\"{label:<12} | {pehe:.4f}{marker}\")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU-BpcGSvoD9",
        "outputId": "2e7f3ccb-f4b3-4cb4-cd8d-a097a568d9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443  (target: < 1.0)\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: MMD=True \n",
            "=======================================================\n",
            "  [MMD] z0=418, z1=542\n",
            "Epoch 000 | loss=10.1364 | recon=1.0492 kl=0.0055 mmd=0.000000\n",
            "  [MMD] z0=439, z1=521\n",
            "Epoch 030 | loss=9.6995 | recon=0.9412 kl=0.0403 mmd=0.000001\n",
            "  [MMD] z0=453, z1=507\n",
            "Epoch 060 | loss=9.8161 | recon=0.9927 kl=0.1363 mmd=0.000001\n",
            "  [MMD] z0=501, z1=459\n",
            "Epoch 090 | loss=10.2237 | recon=1.1241 kl=0.1346 mmd=0.000001\n",
            "  [MMD] z0=482, z1=478\n",
            "Epoch 120 | loss=9.9390 | recon=1.0272 kl=0.1626 mmd=0.000001\n",
            "  [MMD] z0=509, z1=451\n",
            "Epoch 149 | loss=9.8553 | recon=1.0153 kl=0.1617 mmd=0.000001\n",
            "\n",
            "Evaluating MMD=True ...\n",
            "  p0 mean=0.0009  p1 mean=-0.0010\n",
            "  Pred ITE mean=-0.0019  True ITE mean=0.6843\n",
            "PEHE: 3.0819\n",
            "\n",
            "=======================================================\n",
            "Training: MMD=False\n",
            "=======================================================\n",
            "Epoch 000 | loss=10.0247 | recon=1.0151 kl=0.0055 mmd=0.000000\n",
            "Epoch 030 | loss=9.9966 | recon=1.0274 kl=0.0157 mmd=0.000000\n",
            "Epoch 060 | loss=9.8181 | recon=0.9769 kl=0.0217 mmd=0.000000\n",
            "Epoch 090 | loss=10.0562 | recon=1.0491 kl=0.0253 mmd=0.000000\n",
            "Epoch 120 | loss=9.6978 | recon=0.9583 kl=0.0280 mmd=0.000000\n",
            "Epoch 149 | loss=9.9622 | recon=1.0315 kl=0.0245 mmd=0.000000\n",
            "\n",
            "Evaluating MMD=False...\n",
            "  p0 mean=-0.0595  p1 mean=-0.0522\n",
            "  Pred ITE mean=0.0073  True ITE mean=0.6843\n",
            "PEHE: 3.1145\n",
            "\n",
            "=======================================================\n",
            "FINAL ABLATION RESULTS\n",
            "=======================================================\n",
            "Config       | Test PEHE\n",
            "-----------------------------------\n",
            "MMD=True     | 3.0819 ← best\n",
            "MMD=False    | 3.1145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 9\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path         = csv_path\n",
        "        self.batch_size       = batch_size\n",
        "        self.sequence_length  = sequence_length\n",
        "        self.treatment_lag    = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # --- Control treatment T0 ---\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # --- Treated treatment T1: independent additive component ---\n",
        "        # Fix: T1 is NOT 1.5*T0 (that gives correlation=1.0)\n",
        "        # Adding 2*hidden + noise breaks the perfect correlation\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        # --- Regime-Dependent Treated treatment T1 ---\n",
        "        v0 = np.mean(vel) # Using the raw velocity baseline\n",
        "        # High velocity results in a sigmoid approaching 1.0, scaling T0 by 2.0x\n",
        "        # Low velocity results in a sigmoid approaching 0.0, scaling T0 by 1.5x\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1) # Range expanded\n",
        "\n",
        "        # T1 is now a non-linear scaling of T0 based on current ice velocity\n",
        "        #T1_np = ((1.5 + 0.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "        #T1_np = T0_np + 2.0 * hidden + np.random.normal(0, 0.5, T0_np.shape)\n",
        "\n",
        "        # --- Treatment lag (Fix: was duplicate T0 before) ---\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        # Scale and reshape\n",
        "        scaler        = StandardScaler()\n",
        "        X_scaled      = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # --- Counterfactual outcomes ---\n",
        "        # Fix: delta now depends on (T1 - T0) so treatment has a real causal effect\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}  (target: < 1.0)\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices          = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx   = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include T1, Y0, Y1 for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        # Preserves T0/T1 values unchanged\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        # Concatenate treatment with covariates before encoding\n",
        "        x_and_t = torch.cat([X, t], dim=-1)         # (B, T, dim_x+1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)          # (B, T, HIDDEN*2)\n",
        "\n",
        "        mu     = self.fc_mu(h)                        # (B, T, LATENT)\n",
        "        logvar = self.fc_logvar(h)                    # (B, T, LATENT)\n",
        "\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        z   = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "\n",
        "        # Project treatment and concatenate with latent\n",
        "        t_emb  = self.t_proj(t)                       # (B, T, 16)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)       # (B, T, LATENT+16)\n",
        "        y_pred  = self.outcome_head(z_and_t)           # (B, T, 1)\n",
        "\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "# ============================================================\n",
        "# 4. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_model(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)   # KL warmup\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)   # MMD warmup\n",
        "\n",
        "        for batch_idx, (x_in, t_fact, y_fact) in enumerate(train_loader):\n",
        "            x_in, t_fact, y_fact = (\n",
        "                x_in.to(DEVICE), t_fact.to(DEVICE), y_fact.to(DEVICE)\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t_fact, mode='train')\n",
        "\n",
        "            # Reconstruction: factual outcome only\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "\n",
        "            # KL with warmup and clamp to prevent explosion\n",
        "            loss_kl = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            # MMD: balance treated vs control latent representations\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t_fact.view(-1) > t_median).float()\n",
        "                z0     = z_flat[t_flat == 0]\n",
        "                z1     = z_flat[t_flat == 1]\n",
        "\n",
        "                # Print group sizes once per epoch for diagnosis\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0.size(0)}, z1={z1.size(0)}\")\n",
        "\n",
        "                loss_mmd = compute_mmd_stable(z0, z1) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    pehe_sum = 0.0\n",
        "    count    = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x_in, t_f, t_c, y0_gt, y1_gt) in enumerate(test_loader):\n",
        "            x_in = x_in.to(DEVICE)\n",
        "            t_f  = t_f.to(DEVICE)   # T0: control treatment\n",
        "            t_c  = t_c.to(DEVICE)   # T1: treated treatment\n",
        "\n",
        "            # Predict outcome under T0 and T1\n",
        "            p0, _, _, _ = model(x_in, t_f, mode='eval')\n",
        "            p1, _, _, _ = model(x_in, t_c, mode='eval')\n",
        "\n",
        "            ite_pred = p1 - p0\n",
        "            ite_true = (y1_gt - y0_gt).to(DEVICE)\n",
        "\n",
        "            # ITE diagnostic on first batch\n",
        "            if batch_idx == 0:\n",
        "                print(f\"  p0 mean={p0.mean().item():.4f}  \"\n",
        "                      f\"p1 mean={p1.mean().item():.4f}\")\n",
        "                print(f\"  Pred ITE mean={ite_pred.mean().item():.4f}  \"\n",
        "                      f\"True ITE mean={ite_true.mean().item():.4f}\")\n",
        "\n",
        "            pehe_sum += torch.sum((ite_pred - ite_true) ** 2).item()\n",
        "            count    += x_in.size(0) * SEQUENCE_LENGTH\n",
        "\n",
        "    return math.sqrt(pehe_sum / count)\n",
        "\n",
        "# ============================================================\n",
        "# 6. ABLATION BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    # Full ablation: MMD on/off\n",
        "    configs = [True, False]\n",
        "    results = []\n",
        "\n",
        "    for use_mmd in configs:\n",
        "        label = \"MMD=True \" if use_mmd else \"MMD=False\"\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {label}\")\n",
        "        print(f\"{'='*55}\")\n",
        "\n",
        "        model = DCMVAE(use_mmd=use_mmd).to(DEVICE)\n",
        "        train_model(model, train_loader, t_median)\n",
        "\n",
        "        print(f\"\\nEvaluating {label}...\")\n",
        "        pehe = evaluate(model, test_loader)\n",
        "        print(f\"PEHE: {pehe:.4f}\")\n",
        "        results.append((label, pehe))\n",
        "\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(\"FINAL ABLATION RESULTS\")\n",
        "    print(f\"{'='*55}\")\n",
        "    print(f\"{'Config':<12} | {'Test PEHE'}\")\n",
        "    print(\"-\" * 35)\n",
        "    for label, pehe in sorted(results, key=lambda x: x[1]):\n",
        "        marker = \" ← best\" if pehe == min(r[1] for r in results) else \"\"\n",
        "        print(f\"{label:<12} | {pehe:.4f}{marker}\")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxca8_7zwMR6",
        "outputId": "95c163fe-1727-4741-a109-20903f30e005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443  (target: < 1.0)\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: MMD=True \n",
            "=======================================================\n",
            "  [MMD] z0=418, z1=542\n",
            "Epoch 000 | loss=10.1361 | recon=1.0491 kl=0.0055 mmd=0.000000\n",
            "  [MMD] z0=439, z1=521\n",
            "Epoch 030 | loss=9.6986 | recon=0.9412 kl=0.0380 mmd=0.000001\n",
            "  [MMD] z0=453, z1=507\n",
            "Epoch 060 | loss=9.8087 | recon=0.9922 kl=0.1193 mmd=0.000001\n",
            "  [MMD] z0=501, z1=459\n",
            "Epoch 090 | loss=10.2201 | recon=1.1230 kl=0.1224 mmd=0.000002\n",
            "  [MMD] z0=482, z1=478\n",
            "Epoch 120 | loss=9.9264 | recon=1.0252 kl=0.1480 mmd=0.000001\n",
            "  [MMD] z0=509, z1=451\n",
            "Epoch 149 | loss=9.8448 | recon=1.0133 kl=0.1438 mmd=0.000001\n",
            "\n",
            "Evaluating MMD=True ...\n",
            "  p0 mean=0.0049  p1 mean=0.0025\n",
            "  Pred ITE mean=-0.0024  True ITE mean=0.6843\n",
            "PEHE: 3.0795\n",
            "\n",
            "=======================================================\n",
            "Training: MMD=False\n",
            "=======================================================\n",
            "Epoch 000 | loss=10.0251 | recon=1.0152 kl=0.0054 mmd=0.000000\n",
            "Epoch 030 | loss=9.9966 | recon=1.0277 kl=0.0146 mmd=0.000000\n",
            "Epoch 060 | loss=9.8176 | recon=0.9768 kl=0.0200 mmd=0.000000\n",
            "Epoch 090 | loss=10.0611 | recon=1.0499 kl=0.0232 mmd=0.000000\n",
            "Epoch 120 | loss=9.6962 | recon=0.9583 kl=0.0252 mmd=0.000000\n",
            "Epoch 149 | loss=9.9664 | recon=1.0324 kl=0.0233 mmd=0.000000\n",
            "\n",
            "Evaluating MMD=False...\n",
            "  p0 mean=-0.0565  p1 mean=-0.0493\n",
            "  Pred ITE mean=0.0071  True ITE mean=0.6843\n",
            "PEHE: 3.1189\n",
            "\n",
            "=======================================================\n",
            "FINAL ABLATION RESULTS\n",
            "=======================================================\n",
            "Config       | Test PEHE\n",
            "-----------------------------------\n",
            "MMD=True     | 3.0795 ← best\n",
            "MMD=False    | 3.1189\n"
          ]
        }
      ]
    }
  ]
}