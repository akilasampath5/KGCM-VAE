{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVE8tUIdbR6L",
        "outputId": "0b19a63c-d049-4f67-ebcc-e8c0493498d4"
      },
      "id": "NVE8tUIdbR6L",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot8pHCJ4ZfHw",
        "outputId": "7ad73460-e73a-4f92-a541-5b4644b00d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: DCMVAE\n",
            "=======================================================\n",
            "  [MMD] z0=489, z1=471\n",
            "  Epoch 000 | loss=9.8574 | recon=0.9682 kl=0.0052 mmd=0.000000\n",
            "  [MMD] z0=498, z1=462\n",
            "  Epoch 030 | loss=10.0448 | recon=1.0517 kl=0.0401 mmd=0.000001\n",
            "  [MMD] z0=551, z1=409\n",
            "  Epoch 060 | loss=9.9402 | recon=1.0297 kl=0.0988 mmd=0.000001\n",
            "  [MMD] z0=452, z1=508\n",
            "  Epoch 090 | loss=9.8511 | recon=0.9968 kl=0.1253 mmd=0.000002\n",
            "  [MMD] z0=458, z1=502\n",
            "  Epoch 120 | loss=9.6630 | recon=0.9428 kl=0.1034 mmd=0.000001\n",
            "  [MMD] z0=430, z1=530\n",
            "  Epoch 149 | loss=9.8466 | recon=0.9993 kl=0.1126 mmd=0.000001\n",
            "\n",
            "=======================================================\n",
            "Training: R-CRN\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "R-CRN: 100%|██████████| 100/100 [00:19<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: CF-RNN\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CF-RNN: 100%|██████████| 100/100 [00:12<00:00,  7.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: TS-TARNet\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TS-TARNet: 100%|██████████| 100/100 [00:04<00:00, 22.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Model           | Test RMSE    | Test PEHE    \n",
            "----------------------------------------------------------------------\n",
            "DCMVAE          | 0.9897       | 3.0804     \n",
            "R-CRN           | 1.0169       | 3.1424     \n",
            "CF-RNN          | 1.0101       | 3.1167     \n",
            "TS-TARNet       | 1.0281       | 3.1721     \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 9\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path        = csv_path\n",
        "        self.batch_size      = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.treatment_lag   = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # Control treatment T0: SSH-based with seasonal signal\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # Treated treatment T1: regime-dependent scaling using raw velocity\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        v0      = np.mean(vel)\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np   = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "\n",
        "        # Treatment lags\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "        T1_lag_np = self._compute_lag(T1_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        scaler   = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t0_lag    = T0_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t1_lag    = T1_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # Counterfactual outcomes\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite    = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices        = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals (prevents leakage)\n",
        "        # Includes t0_lag for baseline models\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.t_counter[tr_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include y0_cf and y1_cf for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[te_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[te_idx]),\n",
        "            torch.FloatTensor(self.y_factual[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        x_and_t = torch.cat([X, t], dim=-1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)\n",
        "        mu      = self.fc_mu(h)\n",
        "        logvar  = self.fc_logvar(h)\n",
        "        std     = torch.exp(0.5 * logvar)\n",
        "        z       = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "        t_emb   = self.t_proj(t)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)\n",
        "        y_pred  = self.outcome_head(z_and_t)\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "    def counterfactual_prediction(self, X, t0, t1):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p0, _, _, _ = self.forward(X, t0, mode='eval')\n",
        "            p1, _, _, _ = self.forward(X, t1, mode='eval')\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, X, t):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_hat, _, _, _ = self.forward(X, t, mode='eval')\n",
        "        return y_hat.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 4. BASELINE MODELS\n",
        "# ============================================================\n",
        "\n",
        "class R_CRN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class CF_RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class TS_TARNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 5. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_dcmvae(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)\n",
        "\n",
        "        for batch_idx, (x_in, t0, t1, t0_lag, t1_lag, y_fact) in enumerate(train_loader):\n",
        "            x_in, t0, y_fact = x_in.to(DEVICE), t0.to(DEVICE), y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t0, mode='train')\n",
        "\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "            loss_kl    = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t0.view(-1) > t_median).float()\n",
        "                z0_mmd = z_flat[t_flat == 0]\n",
        "                z1_mmd = z_flat[t_flat == 1]\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0_mmd.size(0)}, z1={z1_mmd.size(0)}\")\n",
        "                loss_mmd = compute_mmd_stable(z0_mmd, z1_mmd) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"  Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "\n",
        "def train_baseline(name, model, train_loader):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    model.train()\n",
        "    t_median = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(100), desc=name):\n",
        "        for x_in, t0, t1, t0_lag, t1_lag, y_fact in train_loader:\n",
        "            x_in   = x_in.to(DEVICE)\n",
        "            t0     = t0.to(DEVICE)\n",
        "            t0_lag = t0_lag.to(DEVICE)\n",
        "            y_fact = y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_p, z_s = model(x_in[:, :, :3], t0, t0_lag)\n",
        "            loss_recon = F.mse_loss(y_p, y_fact)\n",
        "            loss_mmd   = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "            if name != 'TS-TARNet':\n",
        "                zf = z_s.reshape(-1, HIDDEN_DIM)\n",
        "                tf = (t0.view(-1) > t_median).float()\n",
        "                z0, z1 = zf[tf == 0], zf[tf == 1]\n",
        "                if z0.size(0) > 1 and z1.size(0) > 1:\n",
        "                    loss_mmd = compute_mmd_stable(z0, z1) * MMD_WEIGHT\n",
        "\n",
        "            loss = loss_recon + loss_mmd\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "# ============================================================\n",
        "# 6. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def calculate_factual_rmse(name, model, loader):\n",
        "    model.eval()\n",
        "    se, count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            yf = yf.to(DEVICE).squeeze(-1)\n",
        "            if name == 'DCMVAE':\n",
        "                yh = model.factual_prediction(x.to(DEVICE), t0.to(DEVICE))\n",
        "            else:\n",
        "                yh = model.factual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE), t0.to(DEVICE), t0l.to(DEVICE)\n",
        "                )\n",
        "            se    += torch.sum((yh - yf) ** 2).item()\n",
        "            count += yf.numel()\n",
        "    return math.sqrt(se / count)\n",
        "\n",
        "\n",
        "def calculate_pehe_ate(name, model, loader):\n",
        "    model.eval()\n",
        "    ite_se, ate_err, count = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            y0t = y0c.to(DEVICE).squeeze(-1)\n",
        "            y1t = y1c.to(DEVICE).squeeze(-1)\n",
        "\n",
        "            if name == 'DCMVAE':\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x.to(DEVICE), t0.to(DEVICE), t1.to(DEVICE)\n",
        "                )\n",
        "            else:\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE),\n",
        "                    t0.to(DEVICE), t1.to(DEVICE),\n",
        "                    t0l.to(DEVICE), t1l.to(DEVICE)\n",
        "                )\n",
        "\n",
        "            ite_h    = y1h - y0h\n",
        "            ite_t    = y1t - y0t\n",
        "            ite_se  += torch.sum((ite_h - ite_t) ** 2).item()\n",
        "            ate_err += torch.sum(ite_h - ite_t).item()\n",
        "            count   += y0t.numel()\n",
        "\n",
        "    return math.sqrt(ite_se / count), abs(ate_err / count)\n",
        "\n",
        "# ============================================================\n",
        "# 7. BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    model_list = {\n",
        "        'DCMVAE'   : DCMVAE(use_mmd=True).to(DEVICE),\n",
        "        'R-CRN'    : R_CRN().to(DEVICE),\n",
        "        'CF-RNN'   : CF_RNN().to(DEVICE),\n",
        "        'TS-TARNet': TS_TARNet().to(DEVICE),\n",
        "    }\n",
        "\n",
        "    # Train all models\n",
        "    for name, model in model_list.items():\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {name}\")\n",
        "        print(f\"{'='*55}\")\n",
        "        if name == 'DCMVAE':\n",
        "            train_dcmvae(model, train_loader, t_median)\n",
        "        else:\n",
        "            train_baseline(name, model, train_loader)\n",
        "\n",
        "    # Evaluate all models\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{'Model':<15} | {'Test RMSE':<12} | {'Test PEHE':<12} \")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for name, model in model_list.items():\n",
        "        rmse        = calculate_factual_rmse(name, model, test_loader)\n",
        "        pehe, ate   = calculate_pehe_ate(name, model, test_loader)\n",
        "        marker      = \" ←\" if name == 'DCMVAE' else \"\"\n",
        "        print(f\"{name:<15} | {rmse:.4f}       | {pehe:.4f}     \")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_benchmark()\n"
      ],
      "id": "ot8pHCJ4ZfHw"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 6\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path        = csv_path\n",
        "        self.batch_size      = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.treatment_lag   = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # Control treatment T0: SSH-based with seasonal signal\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # Treated treatment T1: regime-dependent scaling using raw velocity\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        v0      = np.mean(vel)\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np   = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "\n",
        "        # Treatment lags\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "        T1_lag_np = self._compute_lag(T1_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        scaler   = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t0_lag    = T0_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t1_lag    = T1_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # Counterfactual outcomes\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite    = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices        = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals (prevents leakage)\n",
        "        # Includes t0_lag for baseline models\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.t_counter[tr_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include y0_cf and y1_cf for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[te_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[te_idx]),\n",
        "            torch.FloatTensor(self.y_factual[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        x_and_t = torch.cat([X, t], dim=-1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)\n",
        "        mu      = self.fc_mu(h)\n",
        "        logvar  = self.fc_logvar(h)\n",
        "        std     = torch.exp(0.5 * logvar)\n",
        "        z       = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "        t_emb   = self.t_proj(t)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)\n",
        "        y_pred  = self.outcome_head(z_and_t)\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "    def counterfactual_prediction(self, X, t0, t1):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p0, _, _, _ = self.forward(X, t0, mode='eval')\n",
        "            p1, _, _, _ = self.forward(X, t1, mode='eval')\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, X, t):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_hat, _, _, _ = self.forward(X, t, mode='eval')\n",
        "        return y_hat.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 4. BASELINE MODELS\n",
        "# ============================================================\n",
        "\n",
        "class R_CRN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class CF_RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class TS_TARNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 5. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_dcmvae(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)\n",
        "\n",
        "        for batch_idx, (x_in, t0, t1, t0_lag, t1_lag, y_fact) in enumerate(train_loader):\n",
        "            x_in, t0, y_fact = x_in.to(DEVICE), t0.to(DEVICE), y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t0, mode='train')\n",
        "\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "            loss_kl    = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t0.view(-1) > t_median).float()\n",
        "                z0_mmd = z_flat[t_flat == 0]\n",
        "                z1_mmd = z_flat[t_flat == 1]\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0_mmd.size(0)}, z1={z1_mmd.size(0)}\")\n",
        "                loss_mmd = compute_mmd_stable(z0_mmd, z1_mmd) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"  Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "\n",
        "def train_baseline(name, model, train_loader):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    model.train()\n",
        "    t_median = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(100), desc=name):\n",
        "        for x_in, t0, t1, t0_lag, t1_lag, y_fact in train_loader:\n",
        "            x_in   = x_in.to(DEVICE)\n",
        "            t0     = t0.to(DEVICE)\n",
        "            t0_lag = t0_lag.to(DEVICE)\n",
        "            y_fact = y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_p, z_s = model(x_in[:, :, :3], t0, t0_lag)\n",
        "            loss_recon = F.mse_loss(y_p, y_fact)\n",
        "            loss_mmd   = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "            if name != 'TS-TARNet':\n",
        "                zf = z_s.reshape(-1, HIDDEN_DIM)\n",
        "                tf = (t0.view(-1) > t_median).float()\n",
        "                z0, z1 = zf[tf == 0], zf[tf == 1]\n",
        "                if z0.size(0) > 1 and z1.size(0) > 1:\n",
        "                    loss_mmd = compute_mmd_stable(z0, z1) * MMD_WEIGHT\n",
        "\n",
        "            loss = loss_recon + loss_mmd\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "# ============================================================\n",
        "# 6. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def calculate_factual_rmse(name, model, loader):\n",
        "    model.eval()\n",
        "    se, count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            yf = yf.to(DEVICE).squeeze(-1)\n",
        "            if name == 'DCMVAE':\n",
        "                yh = model.factual_prediction(x.to(DEVICE), t0.to(DEVICE))\n",
        "            else:\n",
        "                yh = model.factual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE), t0.to(DEVICE), t0l.to(DEVICE)\n",
        "                )\n",
        "            se    += torch.sum((yh - yf) ** 2).item()\n",
        "            count += yf.numel()\n",
        "    return math.sqrt(se / count)\n",
        "\n",
        "\n",
        "def calculate_pehe_ate(name, model, loader):\n",
        "    model.eval()\n",
        "    ite_se, ate_err, count = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            y0t = y0c.to(DEVICE).squeeze(-1)\n",
        "            y1t = y1c.to(DEVICE).squeeze(-1)\n",
        "\n",
        "            if name == 'DCMVAE':\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x.to(DEVICE), t0.to(DEVICE), t1.to(DEVICE)\n",
        "                )\n",
        "            else:\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE),\n",
        "                    t0.to(DEVICE), t1.to(DEVICE),\n",
        "                    t0l.to(DEVICE), t1l.to(DEVICE)\n",
        "                )\n",
        "\n",
        "            ite_h    = y1h - y0h\n",
        "            ite_t    = y1t - y0t\n",
        "            ite_se  += torch.sum((ite_h - ite_t) ** 2).item()\n",
        "            ate_err += torch.sum(ite_h - ite_t).item()\n",
        "            count   += y0t.numel()\n",
        "\n",
        "    return math.sqrt(ite_se / count), abs(ate_err / count)\n",
        "\n",
        "# ============================================================\n",
        "# 7. BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    model_list = {\n",
        "        'DCMVAE'   : DCMVAE(use_mmd=True).to(DEVICE),\n",
        "        'R-CRN'    : R_CRN().to(DEVICE),\n",
        "        'CF-RNN'   : CF_RNN().to(DEVICE),\n",
        "        'TS-TARNet': TS_TARNet().to(DEVICE),\n",
        "    }\n",
        "\n",
        "    # Train all models\n",
        "    for name, model in model_list.items():\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {name}\")\n",
        "        print(f\"{'='*55}\")\n",
        "        if name == 'DCMVAE':\n",
        "            train_dcmvae(model, train_loader, t_median)\n",
        "        else:\n",
        "            train_baseline(name, model, train_loader)\n",
        "\n",
        "    # Evaluate all models\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{'Model':<15} | {'Test RMSE':<12} | {'Test PEHE':<12} \")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for name, model in model_list.items():\n",
        "        rmse        = calculate_factual_rmse(name, model, test_loader)\n",
        "        pehe, ate   = calculate_pehe_ate(name, model, test_loader)\n",
        "        marker      = \" ←\" if name == 'DCMVAE' else \"\"\n",
        "        print(f\"{name:<15} | {rmse:.4f}       | {pehe:.4f}      \")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5eHg-ZMcSdo",
        "outputId": "7f9a3bfe-3d4e-4d5c-ebfa-3c3039aa9db9"
      },
      "id": "E5eHg-ZMcSdo",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: DCMVAE\n",
            "=======================================================\n",
            "  [MMD] z0=489, z1=471\n",
            "  Epoch 000 | loss=9.8572 | recon=0.9682 kl=0.0052 mmd=0.000000\n",
            "  [MMD] z0=498, z1=462\n",
            "  Epoch 030 | loss=10.0408 | recon=1.0505 kl=0.0416 mmd=0.000001\n",
            "  [MMD] z0=551, z1=409\n",
            "  Epoch 060 | loss=9.9497 | recon=1.0302 kl=0.0997 mmd=0.000001\n",
            "  [MMD] z0=452, z1=508\n",
            "  Epoch 090 | loss=9.8532 | recon=0.9972 kl=0.1296 mmd=0.000002\n",
            "  [MMD] z0=458, z1=502\n",
            "  Epoch 120 | loss=9.6595 | recon=0.9425 kl=0.1096 mmd=0.000001\n",
            "  [MMD] z0=430, z1=530\n",
            "  Epoch 149 | loss=9.8475 | recon=0.9988 kl=0.1183 mmd=0.000001\n",
            "\n",
            "=======================================================\n",
            "Training: R-CRN\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "R-CRN: 100%|██████████| 100/100 [00:17<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: CF-RNN\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CF-RNN: 100%|██████████| 100/100 [00:12<00:00,  7.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: TS-TARNet\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TS-TARNet: 100%|██████████| 100/100 [00:03<00:00, 27.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Model           | Test RMSE    | Test PEHE    \n",
            "----------------------------------------------------------------------\n",
            "DCMVAE          | 0.9892       | 3.0804      \n",
            "R-CRN           | 1.0156       | 3.1620      \n",
            "CF-RNN          | 1.0099       | 3.1133      \n",
            "TS-TARNet       | 1.0137       | 3.1671      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 3\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path        = csv_path\n",
        "        self.batch_size      = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.treatment_lag   = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # Control treatment T0: SSH-based with seasonal signal\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # Treated treatment T1: regime-dependent scaling using raw velocity\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        v0      = np.mean(vel)\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np   = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "\n",
        "        # Treatment lags\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "        T1_lag_np = self._compute_lag(T1_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        scaler   = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t0_lag    = T0_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t1_lag    = T1_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # Counterfactual outcomes\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite    = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices        = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals (prevents leakage)\n",
        "        # Includes t0_lag for baseline models\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.t_counter[tr_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include y0_cf and y1_cf for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[te_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[te_idx]),\n",
        "            torch.FloatTensor(self.y_factual[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        x_and_t = torch.cat([X, t], dim=-1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)\n",
        "        mu      = self.fc_mu(h)\n",
        "        logvar  = self.fc_logvar(h)\n",
        "        std     = torch.exp(0.5 * logvar)\n",
        "        z       = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "        t_emb   = self.t_proj(t)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)\n",
        "        y_pred  = self.outcome_head(z_and_t)\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "    def counterfactual_prediction(self, X, t0, t1):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p0, _, _, _ = self.forward(X, t0, mode='eval')\n",
        "            p1, _, _, _ = self.forward(X, t1, mode='eval')\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, X, t):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_hat, _, _, _ = self.forward(X, t, mode='eval')\n",
        "        return y_hat.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 4. BASELINE MODELS\n",
        "# ============================================================\n",
        "\n",
        "class R_CRN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class CF_RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class TS_TARNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "        # Treatment projection: same pattern as DCMVAE (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        # Single outcome head conditioned on z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _  = self.rnn(torch.cat([x, t0_lag], dim=2))  # single z\n",
        "        p0    = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1    = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 5. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_dcmvae(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)\n",
        "\n",
        "        for batch_idx, (x_in, t0, t1, t0_lag, t1_lag, y_fact) in enumerate(train_loader):\n",
        "            x_in, t0, y_fact = x_in.to(DEVICE), t0.to(DEVICE), y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t0, mode='train')\n",
        "\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "            loss_kl    = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t0.view(-1) > t_median).float()\n",
        "                z0_mmd = z_flat[t_flat == 0]\n",
        "                z1_mmd = z_flat[t_flat == 1]\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0_mmd.size(0)}, z1={z1_mmd.size(0)}\")\n",
        "                loss_mmd = compute_mmd_stable(z0_mmd, z1_mmd) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"  Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "\n",
        "def train_baseline(name, model, train_loader):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    model.train()\n",
        "    t_median = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(100), desc=name):\n",
        "        for x_in, t0, t1, t0_lag, t1_lag, y_fact in train_loader:\n",
        "            x_in   = x_in.to(DEVICE)\n",
        "            t0     = t0.to(DEVICE)\n",
        "            t0_lag = t0_lag.to(DEVICE)\n",
        "            y_fact = y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_p, z_s = model(x_in[:, :, :3], t0, t0_lag)\n",
        "            loss_recon = F.mse_loss(y_p, y_fact)\n",
        "            loss_mmd   = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "            if name != 'TS-TARNet':\n",
        "                zf = z_s.reshape(-1, HIDDEN_DIM)\n",
        "                tf = (t0.view(-1) > t_median).float()\n",
        "                z0, z1 = zf[tf == 0], zf[tf == 1]\n",
        "                if z0.size(0) > 1 and z1.size(0) > 1:\n",
        "                    loss_mmd = compute_mmd_stable(z0, z1) * MMD_WEIGHT\n",
        "\n",
        "            loss = loss_recon + loss_mmd\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "# ============================================================\n",
        "# 6. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def calculate_factual_rmse(name, model, loader):\n",
        "    model.eval()\n",
        "    se, count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            yf = yf.to(DEVICE).squeeze(-1)\n",
        "            if name == 'DCMVAE':\n",
        "                yh = model.factual_prediction(x.to(DEVICE), t0.to(DEVICE))\n",
        "            else:\n",
        "                yh = model.factual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE), t0.to(DEVICE), t0l.to(DEVICE)\n",
        "                )\n",
        "            se    += torch.sum((yh - yf) ** 2).item()\n",
        "            count += yf.numel()\n",
        "    return math.sqrt(se / count)\n",
        "\n",
        "\n",
        "def calculate_pehe_ate(name, model, loader):\n",
        "    model.eval()\n",
        "    ite_se, ate_err, count = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            y0t = y0c.to(DEVICE).squeeze(-1)\n",
        "            y1t = y1c.to(DEVICE).squeeze(-1)\n",
        "\n",
        "            if name == 'DCMVAE':\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x.to(DEVICE), t0.to(DEVICE), t1.to(DEVICE)\n",
        "                )\n",
        "            else:\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE),\n",
        "                    t0.to(DEVICE), t1.to(DEVICE),\n",
        "                    t0l.to(DEVICE), t1l.to(DEVICE)\n",
        "                )\n",
        "\n",
        "            ite_h    = y1h - y0h\n",
        "            ite_t    = y1t - y0t\n",
        "            ite_se  += torch.sum((ite_h - ite_t) ** 2).item()\n",
        "            ate_err += torch.sum(ite_h - ite_t).item()\n",
        "            count   += y0t.numel()\n",
        "\n",
        "    return math.sqrt(ite_se / count), abs(ate_err / count)\n",
        "\n",
        "# ============================================================\n",
        "# 7. BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    model_list = {\n",
        "        'DCMVAE'   : DCMVAE(use_mmd=True).to(DEVICE),\n",
        "        'R-CRN'    : R_CRN().to(DEVICE),\n",
        "        'CF-RNN'   : CF_RNN().to(DEVICE),\n",
        "        'TS-TARNet': TS_TARNet().to(DEVICE),\n",
        "    }\n",
        "\n",
        "    # Train all models\n",
        "    for name, model in model_list.items():\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {name}\")\n",
        "        print(f\"{'='*55}\")\n",
        "        if name == 'DCMVAE':\n",
        "            train_dcmvae(model, train_loader, t_median)\n",
        "        else:\n",
        "            train_baseline(name, model, train_loader)\n",
        "\n",
        "    # Evaluate all models\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{'Model':<15} | {'Test RMSE':<12} | {'Test PEHE':<12} \")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for name, model in model_list.items():\n",
        "        rmse        = calculate_factual_rmse(name, model, test_loader)\n",
        "        pehe, ate   = calculate_pehe_ate(name, model, test_loader)\n",
        "        marker      = \" ←\" if name == 'DCMVAE' else \"\"\n",
        "        print(f\"{name:<15} | {rmse:.4f}       | {pehe:.4f}       \")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3eAIFgechHo",
        "outputId": "dfb97ede-ffc0-43e1-f74d-edf65627d95e"
      },
      "id": "U3eAIFgechHo",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: DCMVAE\n",
            "=======================================================\n",
            "  [MMD] z0=489, z1=471\n",
            "  Epoch 000 | loss=9.8575 | recon=0.9682 kl=0.0052 mmd=0.000000\n",
            "  [MMD] z0=498, z1=462\n",
            "  Epoch 030 | loss=10.0400 | recon=1.0503 kl=0.0404 mmd=0.000001\n",
            "  [MMD] z0=551, z1=409\n",
            "  Epoch 060 | loss=9.9559 | recon=1.0332 kl=0.0826 mmd=0.000001\n",
            "  [MMD] z0=452, z1=508\n",
            "  Epoch 090 | loss=9.8509 | recon=0.9970 kl=0.1271 mmd=0.000002\n",
            "  [MMD] z0=458, z1=502\n",
            "  Epoch 120 | loss=9.6686 | recon=0.9443 kl=0.1125 mmd=0.000001\n",
            "  [MMD] z0=430, z1=530\n",
            "  Epoch 149 | loss=9.8513 | recon=0.9987 kl=0.1230 mmd=0.000001\n",
            "\n",
            "=======================================================\n",
            "Training: R-CRN\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "R-CRN: 100%|██████████| 100/100 [00:17<00:00,  5.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: CF-RNN\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CF-RNN: 100%|██████████| 100/100 [00:12<00:00,  8.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: TS-TARNet\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TS-TARNet: 100%|██████████| 100/100 [00:04<00:00, 21.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Model           | Test RMSE    | Test PEHE    \n",
            "----------------------------------------------------------------------\n",
            "DCMVAE          | 0.9892       | 3.0782       \n",
            "R-CRN           | 1.0126       | 3.1418       \n",
            "CF-RNN          | 1.0026       | 3.1274       \n",
            "TS-TARNet       | 1.0120       | 3.1292       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}