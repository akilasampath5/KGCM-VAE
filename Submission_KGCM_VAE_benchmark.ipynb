{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVE8tUIdbR6L",
        "outputId": "6cca9ac3-5a5a-4881-cbcb-979901501aa7"
      },
      "id": "NVE8tUIdbR6L",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 3\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path        = csv_path\n",
        "        self.batch_size      = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.treatment_lag   = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # Control treatment T0: SSH-based with seasonal signal\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # Treated treatment T1: regime-dependent scaling using raw velocity\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        v0      = np.mean(vel)\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np   = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "\n",
        "        # Treatment lags\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "        T1_lag_np = self._compute_lag(T1_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        scaler   = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t0_lag    = T0_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t1_lag    = T1_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # Counterfactual outcomes\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite    = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices        = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals (prevents leakage)\n",
        "        # Includes t0_lag for baseline models\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.t_counter[tr_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include y0_cf and y1_cf for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[te_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[te_idx]),\n",
        "            torch.FloatTensor(self.y_factual[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        x_and_t = torch.cat([X, t], dim=-1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)\n",
        "        mu      = self.fc_mu(h)\n",
        "        logvar  = self.fc_logvar(h)\n",
        "        std     = torch.exp(0.5 * logvar)\n",
        "        z       = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "        t_emb   = self.t_proj(t)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)\n",
        "        y_pred  = self.outcome_head(z_and_t)\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "    def counterfactual_prediction(self, X, t0, t1):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p0, _, _, _ = self.forward(X, t0, mode='eval')\n",
        "            p1, _, _, _ = self.forward(X, t1, mode='eval')\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, X, t):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_hat, _, _, _ = self.forward(X, t, mode='eval')\n",
        "        return y_hat.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 4. BASELINE MODELS\n",
        "# ============================================================\n",
        "\n",
        "class Baseline1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.encoder_rnn = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_rnn = nn.GRU(1, HIDDEN_DIM, batch_first=True) # Inputs future treatment\n",
        "\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        # x: (batch, seq, 3), t: (batch, seq, 1), t_lag: (batch, seq, 1)\n",
        "        _, h_n = self.encoder_rnn(torch.cat([x, t_lag], dim=2))\n",
        "\n",
        "        # Decoder uses final hidden state (h_n) and future treatments (t)\n",
        "        # Fix: If h_n is unexpectedly a tuple (e.g., from an LSTM output), take the first element.\n",
        "        # Although encoder_rnn is a GRU and should return a tensor, the error indicates h_n is a tuple.\n",
        "        if isinstance(h_n, tuple):\n",
        "            h_n_for_decoder = h_n[0]\n",
        "        else:\n",
        "            h_n_for_decoder = h_n\n",
        "        z_seq, _ = self.decoder_rnn(t, h_n_for_decoder)\n",
        "\n",
        "        t_emb = self.t_proj(t)\n",
        "        y_pred = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        # Encode history once\n",
        "        _, h_n = self.encoder_rnn(torch.cat([x, t0_lag], dim=2))\n",
        "\n",
        "        # Apply the same fix as in forward method for h_n\n",
        "        if isinstance(h_n, tuple):\n",
        "            h_n_for_decoder = h_n[0]\n",
        "        else:\n",
        "            h_n_for_decoder = h_n\n",
        "\n",
        "        # Decode for treatment path 0\n",
        "        z0, _ = self.decoder_rnn(t0, h_n_for_decoder)\n",
        "        p0 = self.outcome_head(torch.cat([z0, self.t_proj(t0)], dim=-1))\n",
        "\n",
        "        # Decode for treatment path 1\n",
        "        z1, _ = self.decoder_rnn(t1, h_n_for_decoder)\n",
        "        p1 = self.outcome_head(torch.cat([z1, self.t_proj(t1)], dim=-1))\n",
        "\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "class Baseline2(nn.Module):\n",
        "    def __init__(self, n_post=24, nb_features=4, output_dim=1, n_hidden=HIDDEN_DIM):\n",
        "        super().__init__()\n",
        "        self.n_post = n_post\n",
        "\n",
        "        # Encoder: LSTM\n",
        "        self.encoder_rnn = nn.LSTM(nb_features, n_hidden, batch_first=True)\n",
        "\n",
        "        # Decoder: GRU\n",
        "        self.decoder_rnn = nn.GRU(n_hidden, n_hidden, batch_first=True)\n",
        "\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(n_hidden + 16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        # 1. Encode History (x: 3 dims + t_lag: 1 dim = 4 features)\n",
        "        history = torch.cat([x, t_lag], dim=2)\n",
        "        _, (h_n, _) = self.encoder_rnn(history)   # LSTM returns (output, (h_n, c_n))\n",
        "        context_vector = h_n[-1]                   # (batch, n_hidden)\n",
        "\n",
        "        # 2. Repeat context vector across sequence length\n",
        "        batch_size, seq_len, _ = t.size()\n",
        "        repeat_vector = context_vector.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "\n",
        "        # 3. Decode\n",
        "        z_seq, _ = self.decoder_rnn(repeat_vector)\n",
        "\n",
        "        # 4. Predict\n",
        "        t_emb  = self.t_proj(t)\n",
        "        y_pred = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        # Encode history once using factual lag\n",
        "        history = torch.cat([x, t0_lag], dim=2)\n",
        "        _, (h_n, _) = self.encoder_rnn(history)    # LSTM returns (output, (h_n, c_n))\n",
        "        context_vector = h_n[-1]\n",
        "\n",
        "        # Path 0 (factual)\n",
        "        rep0   = context_vector.unsqueeze(1).repeat(1, t0.size(1), 1)\n",
        "        z0, _  = self.decoder_rnn(rep0)\n",
        "        p0     = self.outcome_head(torch.cat([z0, self.t_proj(t0)], dim=-1))\n",
        "\n",
        "        # Path 1 (counterfactual)\n",
        "        rep1   = context_vector.unsqueeze(1).repeat(1, t1.size(1), 1)\n",
        "        z1, _  = self.decoder_rnn(rep1)\n",
        "        p1     = self.outcome_head(torch.cat([z1, self.t_proj(t1)], dim=-1))\n",
        "\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class Baseline3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.LSTM(4, HIDDEN_DIM, batch_first=True)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))  # LSTM: _ = (h_n, c_n)\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _ = self.rnn(torch.cat([x, t0_lag], dim=2))     # LSTM: _ = (h_n, c_n)\n",
        "        p0   = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1   = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 5. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_dcmvae(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)\n",
        "\n",
        "        for batch_idx, (x_in, t0, t1, t0_lag, t1_lag, y_fact) in enumerate(train_loader):\n",
        "            x_in, t0, y_fact = x_in.to(DEVICE), t0.to(DEVICE), y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t0, mode='train')\n",
        "\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "            loss_kl    = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t0.view(-1) > t_median).float()\n",
        "                z0_mmd = z_flat[t_flat == 0]\n",
        "                z1_mmd = z_flat[t_flat == 1]\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0_mmd.size(0)}, z1={z1_mmd.size(0)}\")\n",
        "                loss_mmd = compute_mmd_stable(z0_mmd, z1_mmd) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"  Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "\n",
        "def train_baseline(name, model, train_loader):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    model.train()\n",
        "    t_median = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(100), desc=name):\n",
        "        for x_in, t0, t1, t0_lag, t1_lag, y_fact in train_loader:\n",
        "            x_in   = x_in.to(DEVICE)\n",
        "            t0     = t0.to(DEVICE)\n",
        "            t0_lag = t0_lag.to(DEVICE)\n",
        "            y_fact = y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_p, z_s = model(x_in[:, :, :3], t0, t0_lag)\n",
        "            loss_recon = F.mse_loss(y_p, y_fact)\n",
        "            loss_mmd   = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "            if name != 'TS-TARNet':\n",
        "                zf = z_s.reshape(-1, HIDDEN_DIM)\n",
        "                tf = (t0.view(-1) > t_median).float()\n",
        "                z0, z1 = zf[tf == 0], zf[tf == 1]\n",
        "                if z0.size(0) > 1 and z1.size(0) > 1:\n",
        "                    loss_mmd = compute_mmd_stable(z0, z1) * MMD_WEIGHT\n",
        "\n",
        "            loss = loss_recon + loss_mmd\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "# ============================================================\n",
        "# 6. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def calculate_factual_rmse(name, model, loader):\n",
        "    model.eval()\n",
        "    se, count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            yf = yf.to(DEVICE).squeeze(-1)\n",
        "            if name == 'DCMVAE':\n",
        "                yh = model.factual_prediction(x.to(DEVICE), t0.to(DEVICE))\n",
        "            else:\n",
        "                yh = model.factual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE), t0.to(DEVICE), t0l.to(DEVICE)\n",
        "                )\n",
        "            se    += torch.sum((yh - yf) ** 2).item()\n",
        "            count += yf.numel()\n",
        "    return math.sqrt(se / count)\n",
        "\n",
        "\n",
        "def calculate_pehe_ate(name, model, loader):\n",
        "    model.eval()\n",
        "    ite_se, ate_err, count = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            y0t = y0c.to(DEVICE).squeeze(-1)\n",
        "            y1t = y1c.to(DEVICE).squeeze(-1)\n",
        "\n",
        "            if name == 'DCMVAE':\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x.to(DEVICE), t0.to(DEVICE), t1.to(DEVICE)\n",
        "                )\n",
        "            else:\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE),\n",
        "                    t0.to(DEVICE), t1.to(DEVICE),\n",
        "                    t0l.to(DEVICE), t1l.to(DEVICE)\n",
        "                )\n",
        "\n",
        "            ite_h    = y1h - y0h\n",
        "            ite_t    = y1t - y0t\n",
        "            ite_se  += torch.sum((ite_h - ite_t) ** 2).item()\n",
        "            ate_err += torch.sum(ite_h - ite_t).item()\n",
        "            count   += y0t.numel()\n",
        "\n",
        "    return math.sqrt(ite_se / count), abs(ate_err / count)\n",
        "\n",
        "# ============================================================\n",
        "# 7. BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    model_list = {\n",
        "        'DCMVAE'   : DCMVAE(use_mmd=True).to(DEVICE),\n",
        "        'Baseline1'    : Baseline1().to(DEVICE),\n",
        "        'Baseline2'   : Baseline2().to(DEVICE),\n",
        "        'Baseline3': Baseline3().to(DEVICE),\n",
        "    }\n",
        "\n",
        "    # Train all models\n",
        "    for name, model in model_list.items():\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {name}\")\n",
        "        print(f\"{'='*55}\")\n",
        "        if name == 'DCMVAE':\n",
        "            train_dcmvae(model, train_loader, t_median)\n",
        "        else:\n",
        "            train_baseline(name, model, train_loader)\n",
        "\n",
        "    # Evaluate all models\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{'Model':<15} | {'Test RMSE':<12} | {'Test PEHE':<12} \")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for name, model in model_list.items():\n",
        "        rmse        = calculate_factual_rmse(name, model, test_loader)\n",
        "        pehe, ate   = calculate_pehe_ate(name, model, test_loader)\n",
        "        marker      = \" ←\" if name == 'DCMVAE' else \"\"\n",
        "        print(f\"{name:<15} | {rmse:.4f}       | {pehe:.4f}       \")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3eAIFgechHo",
        "outputId": "2a4cd6d0-1cc7-4205-8197-2e94316f3629"
      },
      "id": "U3eAIFgechHo",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: DCMVAE\n",
            "=======================================================\n",
            "  [MMD] z0=436, z1=524\n",
            "  Epoch 000 | loss=9.9177 | recon=0.9821 kl=0.0058 mmd=0.000000\n",
            "  [MMD] z0=472, z1=488\n",
            "  Epoch 030 | loss=9.9863 | recon=1.0345 kl=0.0348 mmd=0.000001\n",
            "  [MMD] z0=447, z1=513\n",
            "  Epoch 060 | loss=9.5032 | recon=0.8884 kl=0.0772 mmd=0.000001\n",
            "  [MMD] z0=528, z1=432\n",
            "  Epoch 090 | loss=9.9871 | recon=1.0476 kl=0.1439 mmd=0.000002\n",
            "  [MMD] z0=494, z1=466\n",
            "  Epoch 120 | loss=9.8331 | recon=1.0022 kl=0.1120 mmd=0.000001\n",
            "  [MMD] z0=498, z1=462\n",
            "  Epoch 149 | loss=9.8227 | recon=0.9973 kl=0.1202 mmd=0.000002\n",
            "\n",
            "=======================================================\n",
            "Training: Baseline1\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline1: 100%|██████████| 100/100 [00:11<00:00,  8.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: Baseline2\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline2: 100%|██████████| 100/100 [00:09<00:00, 10.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: Baseline3\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline3: 100%|██████████| 100/100 [00:07<00:00, 13.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Model           | Test RMSE    | Test PEHE    \n",
            "----------------------------------------------------------------------\n",
            "DCMVAE          | 0.9883       | 3.0804       \n",
            "Baseline1       | 0.9814       | 3.1353       \n",
            "Baseline2       | 0.9933       | 3.2119       \n",
            "Baseline3       | 0.9994       | 3.1660       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 6\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path        = csv_path\n",
        "        self.batch_size      = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.treatment_lag   = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # Control treatment T0: SSH-based with seasonal signal\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # Treated treatment T1: regime-dependent scaling using raw velocity\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        v0      = np.mean(vel)\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np   = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "\n",
        "        # Treatment lags\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "        T1_lag_np = self._compute_lag(T1_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        scaler   = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t0_lag    = T0_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t1_lag    = T1_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # Counterfactual outcomes\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite    = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices        = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals (prevents leakage)\n",
        "        # Includes t0_lag for baseline models\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.t_counter[tr_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include y0_cf and y1_cf for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[te_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[te_idx]),\n",
        "            torch.FloatTensor(self.y_factual[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        x_and_t = torch.cat([X, t], dim=-1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)\n",
        "        mu      = self.fc_mu(h)\n",
        "        logvar  = self.fc_logvar(h)\n",
        "        std     = torch.exp(0.5 * logvar)\n",
        "        z       = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "        t_emb   = self.t_proj(t)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)\n",
        "        y_pred  = self.outcome_head(z_and_t)\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "    def counterfactual_prediction(self, X, t0, t1):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p0, _, _, _ = self.forward(X, t0, mode='eval')\n",
        "            p1, _, _, _ = self.forward(X, t1, mode='eval')\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, X, t):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_hat, _, _, _ = self.forward(X, t, mode='eval')\n",
        "        return y_hat.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 4. BASELINE MODELS\n",
        "# ============================================================\n",
        "\n",
        "class Baseline1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.encoder_rnn = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_rnn = nn.GRU(1, HIDDEN_DIM, batch_first=True) # Inputs future treatment\n",
        "\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        # x: (batch, seq, 3), t: (batch, seq, 1), t_lag: (batch, seq, 1)\n",
        "        _, h_n = self.encoder_rnn(torch.cat([x, t_lag], dim=2))\n",
        "\n",
        "        # Decoder uses final hidden state (h_n) and future treatments (t)\n",
        "        # Fix: If h_n is unexpectedly a tuple (e.g., from an LSTM output), take the first element.\n",
        "        # Although encoder_rnn is a GRU and should return a tensor, the error indicates h_n is a tuple.\n",
        "        if isinstance(h_n, tuple):\n",
        "            h_n_for_decoder = h_n[0]\n",
        "        else:\n",
        "            h_n_for_decoder = h_n\n",
        "        z_seq, _ = self.decoder_rnn(t, h_n_for_decoder)\n",
        "\n",
        "        t_emb = self.t_proj(t)\n",
        "        y_pred = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        # Encode history once\n",
        "        _, h_n = self.encoder_rnn(torch.cat([x, t0_lag], dim=2))\n",
        "\n",
        "        # Apply the same fix as in forward method for h_n\n",
        "        if isinstance(h_n, tuple):\n",
        "            h_n_for_decoder = h_n[0]\n",
        "        else:\n",
        "            h_n_for_decoder = h_n\n",
        "\n",
        "        # Decode for treatment path 0\n",
        "        z0, _ = self.decoder_rnn(t0, h_n_for_decoder)\n",
        "        p0 = self.outcome_head(torch.cat([z0, self.t_proj(t0)], dim=-1))\n",
        "\n",
        "        # Decode for treatment path 1\n",
        "        z1, _ = self.decoder_rnn(t1, h_n_for_decoder)\n",
        "        p1 = self.outcome_head(torch.cat([z1, self.t_proj(t1)], dim=-1))\n",
        "\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "class Baseline2(nn.Module):\n",
        "    def __init__(self, n_post=24, nb_features=4, output_dim=1, n_hidden=HIDDEN_DIM):\n",
        "        super().__init__()\n",
        "        self.n_post = n_post\n",
        "\n",
        "        # Encoder: LSTM\n",
        "        self.encoder_rnn = nn.LSTM(nb_features, n_hidden, batch_first=True)\n",
        "\n",
        "        # Decoder: GRU\n",
        "        self.decoder_rnn = nn.GRU(n_hidden, n_hidden, batch_first=True)\n",
        "\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(n_hidden + 16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        # 1. Encode History (x: 3 dims + t_lag: 1 dim = 4 features)\n",
        "        history = torch.cat([x, t_lag], dim=2)\n",
        "        _, (h_n, _) = self.encoder_rnn(history)   # LSTM returns (output, (h_n, c_n))\n",
        "        context_vector = h_n[-1]                   # (batch, n_hidden)\n",
        "\n",
        "        # 2. Repeat context vector across sequence length\n",
        "        batch_size, seq_len, _ = t.size()\n",
        "        repeat_vector = context_vector.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "\n",
        "        # 3. Decode\n",
        "        z_seq, _ = self.decoder_rnn(repeat_vector)\n",
        "\n",
        "        # 4. Predict\n",
        "        t_emb  = self.t_proj(t)\n",
        "        y_pred = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        # Encode history once using factual lag\n",
        "        history = torch.cat([x, t0_lag], dim=2)\n",
        "        _, (h_n, _) = self.encoder_rnn(history)    # LSTM returns (output, (h_n, c_n))\n",
        "        context_vector = h_n[-1]\n",
        "\n",
        "        # Path 0 (factual)\n",
        "        rep0   = context_vector.unsqueeze(1).repeat(1, t0.size(1), 1)\n",
        "        z0, _  = self.decoder_rnn(rep0)\n",
        "        p0     = self.outcome_head(torch.cat([z0, self.t_proj(t0)], dim=-1))\n",
        "\n",
        "        # Path 1 (counterfactual)\n",
        "        rep1   = context_vector.unsqueeze(1).repeat(1, t1.size(1), 1)\n",
        "        z1, _  = self.decoder_rnn(rep1)\n",
        "        p1     = self.outcome_head(torch.cat([z1, self.t_proj(t1)], dim=-1))\n",
        "\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class Baseline3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.LSTM(4, HIDDEN_DIM, batch_first=True)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))  # LSTM: _ = (h_n, c_n)\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _ = self.rnn(torch.cat([x, t0_lag], dim=2))     # LSTM: _ = (h_n, c_n)\n",
        "        p0   = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1   = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 5. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_dcmvae(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)\n",
        "\n",
        "        for batch_idx, (x_in, t0, t1, t0_lag, t1_lag, y_fact) in enumerate(train_loader):\n",
        "            x_in, t0, y_fact = x_in.to(DEVICE), t0.to(DEVICE), y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t0, mode='train')\n",
        "\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "            loss_kl    = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t0.view(-1) > t_median).float()\n",
        "                z0_mmd = z_flat[t_flat == 0]\n",
        "                z1_mmd = z_flat[t_flat == 1]\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0_mmd.size(0)}, z1={z1_mmd.size(0)}\")\n",
        "                loss_mmd = compute_mmd_stable(z0_mmd, z1_mmd) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"  Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "\n",
        "def train_baseline(name, model, train_loader):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    model.train()\n",
        "    t_median = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(100), desc=name):\n",
        "        for x_in, t0, t1, t0_lag, t1_lag, y_fact in train_loader:\n",
        "            x_in   = x_in.to(DEVICE)\n",
        "            t0     = t0.to(DEVICE)\n",
        "            t0_lag = t0_lag.to(DEVICE)\n",
        "            y_fact = y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_p, z_s = model(x_in[:, :, :3], t0, t0_lag)\n",
        "            loss_recon = F.mse_loss(y_p, y_fact)\n",
        "            loss_mmd   = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "            if name != 'TS-TARNet':\n",
        "                zf = z_s.reshape(-1, HIDDEN_DIM)\n",
        "                tf = (t0.view(-1) > t_median).float()\n",
        "                z0, z1 = zf[tf == 0], zf[tf == 1]\n",
        "                if z0.size(0) > 1 and z1.size(0) > 1:\n",
        "                    loss_mmd = compute_mmd_stable(z0, z1) * MMD_WEIGHT\n",
        "\n",
        "            loss = loss_recon + loss_mmd\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "# ============================================================\n",
        "# 6. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def calculate_factual_rmse(name, model, loader):\n",
        "    model.eval()\n",
        "    se, count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            yf = yf.to(DEVICE).squeeze(-1)\n",
        "            if name == 'DCMVAE':\n",
        "                yh = model.factual_prediction(x.to(DEVICE), t0.to(DEVICE))\n",
        "            else:\n",
        "                yh = model.factual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE), t0.to(DEVICE), t0l.to(DEVICE)\n",
        "                )\n",
        "            se    += torch.sum((yh - yf) ** 2).item()\n",
        "            count += yf.numel()\n",
        "    return math.sqrt(se / count)\n",
        "\n",
        "\n",
        "def calculate_pehe_ate(name, model, loader):\n",
        "    model.eval()\n",
        "    ite_se, ate_err, count = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            y0t = y0c.to(DEVICE).squeeze(-1)\n",
        "            y1t = y1c.to(DEVICE).squeeze(-1)\n",
        "\n",
        "            if name == 'DCMVAE':\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x.to(DEVICE), t0.to(DEVICE), t1.to(DEVICE)\n",
        "                )\n",
        "            else:\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE),\n",
        "                    t0.to(DEVICE), t1.to(DEVICE),\n",
        "                    t0l.to(DEVICE), t1l.to(DEVICE)\n",
        "                )\n",
        "\n",
        "            ite_h    = y1h - y0h\n",
        "            ite_t    = y1t - y0t\n",
        "            ite_se  += torch.sum((ite_h - ite_t) ** 2).item()\n",
        "            ate_err += torch.sum(ite_h - ite_t).item()\n",
        "            count   += y0t.numel()\n",
        "\n",
        "    return math.sqrt(ite_se / count), abs(ate_err / count)\n",
        "\n",
        "# ============================================================\n",
        "# 7. BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    model_list = {\n",
        "        'DCMVAE'   : DCMVAE(use_mmd=True).to(DEVICE),\n",
        "        'Baseline1'    : Baseline1().to(DEVICE),\n",
        "        'Baseline2'   : Baseline2().to(DEVICE),\n",
        "        'Baseline3': Baseline3().to(DEVICE),\n",
        "    }\n",
        "\n",
        "    # Train all models\n",
        "    for name, model in model_list.items():\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {name}\")\n",
        "        print(f\"{'='*55}\")\n",
        "        if name == 'DCMVAE':\n",
        "            train_dcmvae(model, train_loader, t_median)\n",
        "        else:\n",
        "            train_baseline(name, model, train_loader)\n",
        "\n",
        "    # Evaluate all models\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{'Model':<15} | {'Test RMSE':<12} | {'Test PEHE':<12} \")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for name, model in model_list.items():\n",
        "        rmse        = calculate_factual_rmse(name, model, test_loader)\n",
        "        pehe, ate   = calculate_pehe_ate(name, model, test_loader)\n",
        "        marker      = \" ←\" if name == 'DCMVAE' else \"\"\n",
        "        print(f\"{name:<15} | {rmse:.4f}       | {pehe:.4f}       \")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_benchmark()"
      ],
      "metadata": {
        "id": "Xk1IYvK47a_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d179f834-2360-4d0d-93b2-fe091d8e626f"
      },
      "id": "Xk1IYvK47a_L",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: DCMVAE\n",
            "=======================================================\n",
            "  [MMD] z0=436, z1=524\n",
            "  Epoch 000 | loss=9.9184 | recon=0.9822 kl=0.0058 mmd=0.000000\n",
            "  [MMD] z0=472, z1=488\n",
            "  Epoch 030 | loss=9.9840 | recon=1.0343 kl=0.0357 mmd=0.000001\n",
            "  [MMD] z0=447, z1=513\n",
            "  Epoch 060 | loss=9.4980 | recon=0.8875 kl=0.0850 mmd=0.000001\n",
            "  [MMD] z0=528, z1=432\n",
            "  Epoch 090 | loss=9.9922 | recon=1.0483 kl=0.1582 mmd=0.000002\n",
            "  [MMD] z0=494, z1=466\n",
            "  Epoch 120 | loss=9.8228 | recon=1.0004 kl=0.1181 mmd=0.000001\n",
            "  [MMD] z0=498, z1=462\n",
            "  Epoch 149 | loss=9.8191 | recon=0.9969 kl=0.1279 mmd=0.000002\n",
            "\n",
            "=======================================================\n",
            "Training: Baseline1\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline1: 100%|██████████| 100/100 [00:11<00:00,  8.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: Baseline2\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline2: 100%|██████████| 100/100 [00:11<00:00,  8.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: Baseline3\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline3: 100%|██████████| 100/100 [00:07<00:00, 13.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Model           | Test RMSE    | Test PEHE    \n",
            "----------------------------------------------------------------------\n",
            "DCMVAE          | 0.9884       | 3.0816       \n",
            "Baseline1       | 0.9792       | 3.1583       \n",
            "Baseline2       | 0.9928       | 3.2136       \n",
            "Baseline3       | 1.0004       | 3.1683       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 0. CONFIG & REPRODUCIBILITY\n",
        "# ============================================================\n",
        "\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "random.seed(REPRODUCIBILITY_SEED)\n",
        "np.random.seed(REPRODUCIBILITY_SEED)\n",
        "torch.manual_seed(REPRODUCIBILITY_SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(REPRODUCIBILITY_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "dim_x_features  = 5        # updated dynamically from data\n",
        "SEQUENCE_LENGTH  = 30\n",
        "BATCH_SIZE       = 32\n",
        "HIDDEN_DIM       = 128\n",
        "LATENT_DIM       = 64\n",
        "NUM_EPOCHS       = 150\n",
        "LEARNING_RATE    = 5e-4\n",
        "KL_WEIGHT        = 0.001\n",
        "MMD_WEIGHT       = 1.0\n",
        "WINDOW_SIZE      = 5\n",
        "TREATMENT_LAG    = 9\n",
        "\n",
        "CSV_FILE_PATH = \"arctic_s2s_multivar_2020_2024.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. MMD UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def gaussian_rbf_matrix(x, y, sigma=1.0):\n",
        "    x_norm = (x ** 2).sum(1).unsqueeze(1)\n",
        "    y_norm = (y ** 2).sum(1).unsqueeze(0)\n",
        "    dists  = x_norm + y_norm - 2 * (x @ y.t())\n",
        "    return torch.exp(-dists / (2 * sigma**2 + 1e-12))\n",
        "\n",
        "def compute_mmd_stable(x, y):\n",
        "    if x is None or y is None or x.size(0) <= 1 or y.size(0) <= 1:\n",
        "        return torch.tensor(0.0, device=DEVICE)\n",
        "    mmd = 0.0\n",
        "    for sigma in [0.5, 1.0, 2.0]:\n",
        "        K_xx = gaussian_rbf_matrix(x, x, sigma)\n",
        "        K_yy = gaussian_rbf_matrix(y, y, sigma)\n",
        "        K_xy = gaussian_rbf_matrix(x, y, sigma)\n",
        "        n, m = x.size(0), y.size(0)\n",
        "        sum_xx = (K_xx.sum() - torch.diag(K_xx).sum()) / (n * (n - 1))\n",
        "        sum_yy = (K_yy.sum() - torch.diag(K_yy).sum()) / (m * (m - 1))\n",
        "        sum_xy = K_xy.mean()\n",
        "        mmd += sum_xx + sum_yy - 2.0 * sum_xy\n",
        "    return mmd / 3.0\n",
        "\n",
        "# ============================================================\n",
        "# 2. DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "class IHDP_TimeSeries:\n",
        "\n",
        "    def __init__(self, csv_path, batch_size, sequence_length,\n",
        "                 treatment_lag=TREATMENT_LAG):\n",
        "        self.csv_path        = csv_path\n",
        "        self.batch_size      = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.treatment_lag   = treatment_lag\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_moving_window(series, window_size):\n",
        "        return pd.Series(series.flatten()).rolling(\n",
        "            window=window_size, min_periods=1\n",
        "        ).mean().values.reshape(-1, 1)\n",
        "\n",
        "    def _compute_lag(self, T, lag):\n",
        "        \"\"\"Shift treatment by `lag` steps; fill leading entries with zero.\"\"\"\n",
        "        Tlag = np.zeros_like(T)\n",
        "        Tlag[lag:] = T[:-lag]\n",
        "        return Tlag\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        if os.path.exists(self.csv_path):\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "        else:\n",
        "            df = pd.DataFrame(\n",
        "                np.random.randn(1621, 5),\n",
        "                columns=['uoe', 'von', 'total_vel', 'zos', 'sithick']\n",
        "            )\n",
        "\n",
        "        x_base = df[['uoe', 'von', 'total_vel']].values\n",
        "        y_base = df[['sithick']].values\n",
        "        ssh    = df['zos'].values.reshape(-1, 1)\n",
        "        vel    = df['total_vel'].values.reshape(-1, 1)\n",
        "        hidden = np.sin(np.linspace(0, 30 * np.pi, len(df))).reshape(-1, 1)\n",
        "\n",
        "        # Control treatment T0: SSH-based with seasonal signal\n",
        "        T0_smooth = self.apply_moving_window(ssh, WINDOW_SIZE)\n",
        "        T0_np     = T0_smooth + (2.0 * hidden) + np.random.normal(0, 0.1, ssh.shape)\n",
        "\n",
        "        # Treated treatment T1: regime-dependent scaling using raw velocity\n",
        "        np.random.seed(REPRODUCIBILITY_SEED)\n",
        "        v0      = np.mean(vel)\n",
        "        sigmoid = 1 / (1 + np.exp(-(-5.0) * (vel - v0)))\n",
        "        T1_np   = ((1.0 + 1.5 * sigmoid) * T0_np).reshape(-1, 1)\n",
        "\n",
        "        # Treatment lags\n",
        "        T0_lag_np = self._compute_lag(T0_np, self.treatment_lag)\n",
        "        T1_lag_np = self._compute_lag(T1_np, self.treatment_lag)\n",
        "\n",
        "        # Covariates: 3 base + T0 + T0_lag = 5 features\n",
        "        X_RAW = np.concatenate([x_base, T0_np, T0_lag_np], axis=1)\n",
        "\n",
        "        num_seq = len(df) // self.sequence_length\n",
        "        limit   = num_seq * self.sequence_length\n",
        "\n",
        "        # Update dim_x_features dynamically\n",
        "        global dim_x_features\n",
        "        dim_x_features = X_RAW.shape[1]\n",
        "\n",
        "        scaler   = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_RAW[:limit])\n",
        "\n",
        "        self.xall      = X_scaled.reshape(num_seq, self.sequence_length, dim_x_features)\n",
        "        self.t_factual = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t_counter = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t0_lag    = T0_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.t1_lag    = T1_lag_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        self.y_factual = y_base[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "\n",
        "        # Counterfactual outcomes\n",
        "        hidden_seq = hidden[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T0_seq     = T0_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        T1_seq     = T1_np[:limit].reshape(num_seq, self.sequence_length, 1)\n",
        "        delta      = -6.0 * np.abs(hidden_seq) * np.tanh(2.0 * (T1_seq - T0_seq))\n",
        "        self.y0_cf = self.y_factual\n",
        "        self.y1_cf = self.y_factual + delta\n",
        "\n",
        "        # Diagnostics\n",
        "        ite    = self.y1_cf - self.y0_cf\n",
        "        t_corr = np.corrcoef(T0_np.flatten(), T1_np.flatten())[0, 1]\n",
        "        print(\"=\" * 55)\n",
        "        print(\"DATA DIAGNOSTICS\")\n",
        "        print(\"=\" * 55)\n",
        "        print(f\"Num sequences:      {self.xall.shape[0]}\")\n",
        "        print(f\"Sequence length:    {self.xall.shape[1]}\")\n",
        "        print(f\"Feature dim:        {self.xall.shape[2]}\")\n",
        "        print(f\"Mean |ITE|:         {np.abs(ite).mean():.4f}\")\n",
        "        print(f\"Std  ITE:           {ite.std():.4f}\")\n",
        "        print(f\"% near-zero ITE:    {(np.abs(ite) < 0.01).mean()*100:.1f}%\")\n",
        "        print(f\"T0-T1 correlation:  {t_corr:.4f}\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        # Split indices ONCE to guarantee alignment across all arrays\n",
        "        indices        = np.arange(len(self.xall))\n",
        "        tr_idx, te_idx = train_test_split(\n",
        "            indices, test_size=0.2, random_state=REPRODUCIBILITY_SEED\n",
        "        )\n",
        "\n",
        "        # Train: factual only — no counterfactuals (prevents leakage)\n",
        "        # Includes t0_lag for baseline models\n",
        "        train_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[tr_idx]),\n",
        "            torch.FloatTensor(self.t_factual[tr_idx]),\n",
        "            torch.FloatTensor(self.t_counter[tr_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[tr_idx]),\n",
        "            torch.FloatTensor(self.y_factual[tr_idx])\n",
        "        )\n",
        "\n",
        "        # Test: include y0_cf and y1_cf for PEHE evaluation only\n",
        "        test_ds = TensorDataset(\n",
        "            torch.FloatTensor(self.xall[te_idx]),\n",
        "            torch.FloatTensor(self.t_factual[te_idx]),\n",
        "            torch.FloatTensor(self.t_counter[te_idx]),\n",
        "            torch.FloatTensor(self.t0_lag[te_idx]),\n",
        "            torch.FloatTensor(self.t1_lag[te_idx]),\n",
        "            torch.FloatTensor(self.y_factual[te_idx]),\n",
        "            torch.FloatTensor(self.y0_cf[te_idx]),\n",
        "            torch.FloatTensor(self.y1_cf[te_idx])\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, BATCH_SIZE, shuffle=True),\n",
        "            DataLoader(test_ds,  BATCH_SIZE, shuffle=False)\n",
        "        )\n",
        "\n",
        "# ============================================================\n",
        "# 3. DCMVAE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class DCMVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, use_mmd=True):\n",
        "        super().__init__()\n",
        "        self.use_mmd = use_mmd\n",
        "\n",
        "        # Encoder receives covariates + treatment so latent space\n",
        "        # learns treatment-dependent representations for MMD balancing\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            dim_x_features + 1, HIDDEN_DIM,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "        self.fc_mu     = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "        self.fc_logvar = nn.Linear(HIDDEN_DIM * 2, LATENT_DIM)\n",
        "\n",
        "        # Treatment projection: amplifies treatment signal (1 → 16 dims)\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Outcome head conditioned on latent z + projected treatment\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(LATENT_DIM + 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, t, mode='train'):\n",
        "        x_and_t = torch.cat([X, t], dim=-1)\n",
        "        h, _    = self.encoder_rnn(x_and_t)\n",
        "        mu      = self.fc_mu(h)\n",
        "        logvar  = self.fc_logvar(h)\n",
        "        std     = torch.exp(0.5 * logvar)\n",
        "        z       = mu + torch.randn_like(mu) * std if mode == 'train' else mu\n",
        "        t_emb   = self.t_proj(t)\n",
        "        z_and_t = torch.cat([z, t_emb], dim=-1)\n",
        "        y_pred  = self.outcome_head(z_and_t)\n",
        "        return y_pred, mu, logvar, z\n",
        "\n",
        "    def counterfactual_prediction(self, X, t0, t1):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p0, _, _, _ = self.forward(X, t0, mode='eval')\n",
        "            p1, _, _, _ = self.forward(X, t1, mode='eval')\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, X, t):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_hat, _, _, _ = self.forward(X, t, mode='eval')\n",
        "        return y_hat.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 4. BASELINE MODELS\n",
        "# ============================================================\n",
        "\n",
        "class Baseline1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.encoder_rnn = nn.GRU(4, HIDDEN_DIM, batch_first=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_rnn = nn.GRU(1, HIDDEN_DIM, batch_first=True) # Inputs future treatment\n",
        "\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        # x: (batch, seq, 3), t: (batch, seq, 1), t_lag: (batch, seq, 1)\n",
        "        _, h_n = self.encoder_rnn(torch.cat([x, t_lag], dim=2))\n",
        "\n",
        "        # Decoder uses final hidden state (h_n) and future treatments (t)\n",
        "        # Fix: If h_n is unexpectedly a tuple (e.g., from an LSTM output), take the first element.\n",
        "        # Although encoder_rnn is a GRU and should return a tensor, the error indicates h_n is a tuple.\n",
        "        if isinstance(h_n, tuple):\n",
        "            h_n_for_decoder = h_n[0]\n",
        "        else:\n",
        "            h_n_for_decoder = h_n\n",
        "        z_seq, _ = self.decoder_rnn(t, h_n_for_decoder)\n",
        "\n",
        "        t_emb = self.t_proj(t)\n",
        "        y_pred = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        # Encode history once\n",
        "        _, h_n = self.encoder_rnn(torch.cat([x, t0_lag], dim=2))\n",
        "\n",
        "        # Apply the same fix as in forward method for h_n\n",
        "        if isinstance(h_n, tuple):\n",
        "            h_n_for_decoder = h_n[0]\n",
        "        else:\n",
        "            h_n_for_decoder = h_n\n",
        "\n",
        "        # Decode for treatment path 0\n",
        "        z0, _ = self.decoder_rnn(t0, h_n_for_decoder)\n",
        "        p0 = self.outcome_head(torch.cat([z0, self.t_proj(t0)], dim=-1))\n",
        "\n",
        "        # Decode for treatment path 1\n",
        "        z1, _ = self.decoder_rnn(t1, h_n_for_decoder)\n",
        "        p1 = self.outcome_head(torch.cat([z1, self.t_proj(t1)], dim=-1))\n",
        "\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "class Baseline2(nn.Module):\n",
        "    def __init__(self, n_post=24, nb_features=4, output_dim=1, n_hidden=HIDDEN_DIM):\n",
        "        super().__init__()\n",
        "        self.n_post = n_post\n",
        "\n",
        "        # Encoder: LSTM\n",
        "        self.encoder_rnn = nn.LSTM(nb_features, n_hidden, batch_first=True)\n",
        "\n",
        "        # Decoder: GRU\n",
        "        self.decoder_rnn = nn.GRU(n_hidden, n_hidden, batch_first=True)\n",
        "\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(n_hidden + 16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        # 1. Encode History (x: 3 dims + t_lag: 1 dim = 4 features)\n",
        "        history = torch.cat([x, t_lag], dim=2)\n",
        "        _, (h_n, _) = self.encoder_rnn(history)   # LSTM returns (output, (h_n, c_n))\n",
        "        context_vector = h_n[-1]                   # (batch, n_hidden)\n",
        "\n",
        "        # 2. Repeat context vector across sequence length\n",
        "        batch_size, seq_len, _ = t.size()\n",
        "        repeat_vector = context_vector.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "\n",
        "        # 3. Decode\n",
        "        z_seq, _ = self.decoder_rnn(repeat_vector)\n",
        "\n",
        "        # 4. Predict\n",
        "        t_emb  = self.t_proj(t)\n",
        "        y_pred = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        # Encode history once using factual lag\n",
        "        history = torch.cat([x, t0_lag], dim=2)\n",
        "        _, (h_n, _) = self.encoder_rnn(history)    # LSTM returns (output, (h_n, c_n))\n",
        "        context_vector = h_n[-1]\n",
        "\n",
        "        # Path 0 (factual)\n",
        "        rep0   = context_vector.unsqueeze(1).repeat(1, t0.size(1), 1)\n",
        "        z0, _  = self.decoder_rnn(rep0)\n",
        "        p0     = self.outcome_head(torch.cat([z0, self.t_proj(t0)], dim=-1))\n",
        "\n",
        "        # Path 1 (counterfactual)\n",
        "        rep1   = context_vector.unsqueeze(1).repeat(1, t1.size(1), 1)\n",
        "        z1, _  = self.decoder_rnn(rep1)\n",
        "        p1     = self.outcome_head(torch.cat([z1, self.t_proj(t1)], dim=-1))\n",
        "\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "\n",
        "class Baseline3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn    = nn.LSTM(4, HIDDEN_DIM, batch_first=True)\n",
        "        self.t_proj = nn.Sequential(nn.Linear(1, 16), nn.ReLU())\n",
        "        self.outcome_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM + 16, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, t_lag):\n",
        "        z_seq, _ = self.rnn(torch.cat([x, t_lag], dim=2))  # LSTM: _ = (h_n, c_n)\n",
        "        t_emb    = self.t_proj(t)\n",
        "        y_pred   = self.outcome_head(torch.cat([z_seq, t_emb], dim=-1))\n",
        "        return y_pred, z_seq\n",
        "\n",
        "    def counterfactual_prediction(self, x, t0, t1, t0_lag, t1_lag):\n",
        "        z, _ = self.rnn(torch.cat([x, t0_lag], dim=2))     # LSTM: _ = (h_n, c_n)\n",
        "        p0   = self.outcome_head(torch.cat([z, self.t_proj(t0)], dim=-1))\n",
        "        p1   = self.outcome_head(torch.cat([z, self.t_proj(t1)], dim=-1))\n",
        "        return p0.squeeze(-1), p1.squeeze(-1)\n",
        "\n",
        "    def factual_prediction(self, x, t, t_lag):\n",
        "        y_h, _ = self.forward(x, t, t_lag)\n",
        "        return y_h.squeeze(-1)\n",
        "\n",
        "# ============================================================\n",
        "# 5. TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train_dcmvae(model, train_loader, t_median):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        kl_scale   = min(1.0, epoch / 30.0)\n",
        "        mmd_scale  = min(1.0, epoch / 50.0)\n",
        "\n",
        "        for batch_idx, (x_in, t0, t1, t0_lag, t1_lag, y_fact) in enumerate(train_loader):\n",
        "            x_in, t0, y_fact = x_in.to(DEVICE), t0.to(DEVICE), y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, mu, logvar, z = model(x_in, t0, mode='train')\n",
        "\n",
        "            loss_recon = F.mse_loss(y_pred, y_fact)\n",
        "            loss_kl    = torch.clamp(\n",
        "                -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()),\n",
        "                max=1.0\n",
        "            )\n",
        "\n",
        "            loss_mmd = torch.tensor(0.0, device=DEVICE)\n",
        "            if model.use_mmd:\n",
        "                z_flat = z.view(-1, LATENT_DIM)\n",
        "                t_flat = (t0.view(-1) > t_median).float()\n",
        "                z0_mmd = z_flat[t_flat == 0]\n",
        "                z1_mmd = z_flat[t_flat == 1]\n",
        "                if batch_idx == 0 and (epoch % 30 == 0 or epoch == NUM_EPOCHS - 1):\n",
        "                    print(f\"  [MMD] z0={z0_mmd.size(0)}, z1={z1_mmd.size(0)}\")\n",
        "                loss_mmd = compute_mmd_stable(z0_mmd, z1_mmd) * mmd_scale\n",
        "\n",
        "            loss = (10.0 * loss_recon\n",
        "                    + KL_WEIGHT * kl_scale * loss_kl\n",
        "                    + MMD_WEIGHT * loss_mmd)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        scheduler.step(avg)\n",
        "        if epoch % 30 == 0 or epoch == NUM_EPOCHS - 1:\n",
        "            print(f\"  Epoch {epoch:03d} | loss={avg:.4f} | \"\n",
        "                  f\"recon={loss_recon.item():.4f} \"\n",
        "                  f\"kl={loss_kl.item():.4f} \"\n",
        "                  f\"mmd={loss_mmd.item():.6f}\")\n",
        "\n",
        "\n",
        "def train_baseline(name, model, train_loader):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
        "    )\n",
        "    model.train()\n",
        "    t_median = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(100), desc=name):\n",
        "        for x_in, t0, t1, t0_lag, t1_lag, y_fact in train_loader:\n",
        "            x_in   = x_in.to(DEVICE)\n",
        "            t0     = t0.to(DEVICE)\n",
        "            t0_lag = t0_lag.to(DEVICE)\n",
        "            y_fact = y_fact.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_p, z_s = model(x_in[:, :, :3], t0, t0_lag)\n",
        "            loss_recon = F.mse_loss(y_p, y_fact)\n",
        "            loss_mmd   = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "            if name != 'TS-TARNet':\n",
        "                zf = z_s.reshape(-1, HIDDEN_DIM)\n",
        "                tf = (t0.view(-1) > t_median).float()\n",
        "                z0, z1 = zf[tf == 0], zf[tf == 1]\n",
        "                if z0.size(0) > 1 and z1.size(0) > 1:\n",
        "                    loss_mmd = compute_mmd_stable(z0, z1) * MMD_WEIGHT\n",
        "\n",
        "            loss = loss_recon + loss_mmd\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "# ============================================================\n",
        "# 6. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def calculate_factual_rmse(name, model, loader):\n",
        "    model.eval()\n",
        "    se, count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            yf = yf.to(DEVICE).squeeze(-1)\n",
        "            if name == 'DCMVAE':\n",
        "                yh = model.factual_prediction(x.to(DEVICE), t0.to(DEVICE))\n",
        "            else:\n",
        "                yh = model.factual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE), t0.to(DEVICE), t0l.to(DEVICE)\n",
        "                )\n",
        "            se    += torch.sum((yh - yf) ** 2).item()\n",
        "            count += yf.numel()\n",
        "    return math.sqrt(se / count)\n",
        "\n",
        "\n",
        "def calculate_pehe_ate(name, model, loader):\n",
        "    model.eval()\n",
        "    ite_se, ate_err, count = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, t0, t1, t0l, t1l, yf, y0c, y1c in loader:\n",
        "            y0t = y0c.to(DEVICE).squeeze(-1)\n",
        "            y1t = y1c.to(DEVICE).squeeze(-1)\n",
        "\n",
        "            if name == 'DCMVAE':\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x.to(DEVICE), t0.to(DEVICE), t1.to(DEVICE)\n",
        "                )\n",
        "            else:\n",
        "                y0h, y1h = model.counterfactual_prediction(\n",
        "                    x[:, :, :3].to(DEVICE),\n",
        "                    t0.to(DEVICE), t1.to(DEVICE),\n",
        "                    t0l.to(DEVICE), t1l.to(DEVICE)\n",
        "                )\n",
        "\n",
        "            ite_h    = y1h - y0h\n",
        "            ite_t    = y1t - y0t\n",
        "            ite_se  += torch.sum((ite_h - ite_t) ** 2).item()\n",
        "            ate_err += torch.sum(ite_h - ite_t).item()\n",
        "            count   += y0t.numel()\n",
        "\n",
        "    return math.sqrt(ite_se / count), abs(ate_err / count)\n",
        "\n",
        "# ============================================================\n",
        "# 7. BENCHMARK\n",
        "# ============================================================\n",
        "\n",
        "def run_benchmark():\n",
        "    dm = IHDP_TimeSeries(CSV_FILE_PATH, BATCH_SIZE, SEQUENCE_LENGTH)\n",
        "    train_loader, test_loader = dm.get_dataloaders()\n",
        "    t_median = float(np.median(dm.t_factual))\n",
        "\n",
        "    model_list = {\n",
        "        'DCMVAE'   : DCMVAE(use_mmd=True).to(DEVICE),\n",
        "        'Baseline1'    : Baseline1().to(DEVICE),\n",
        "        'Baseline2'   : Baseline2().to(DEVICE),\n",
        "        'Baseline3': Baseline3().to(DEVICE),\n",
        "    }\n",
        "\n",
        "    # Train all models\n",
        "    for name, model in model_list.items():\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Training: {name}\")\n",
        "        print(f\"{'='*55}\")\n",
        "        if name == 'DCMVAE':\n",
        "            train_dcmvae(model, train_loader, t_median)\n",
        "        else:\n",
        "            train_baseline(name, model, train_loader)\n",
        "\n",
        "    # Evaluate all models\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{'Model':<15} | {'Test RMSE':<12} | {'Test PEHE':<12} \")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for name, model in model_list.items():\n",
        "        rmse        = calculate_factual_rmse(name, model, test_loader)\n",
        "        pehe, ate   = calculate_pehe_ate(name, model, test_loader)\n",
        "        marker      = \" ←\" if name == 'DCMVAE' else \"\"\n",
        "        print(f\"{name:<15} | {rmse:.4f}       | {pehe:.4f}       \")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94ngP6DFGqne",
        "outputId": "4ce5f238-a107-4de1-e2e1-b1e6a2075e72"
      },
      "id": "94ngP6DFGqne",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "DATA DIAGNOSTICS\n",
            "=======================================================\n",
            "Num sequences:      54\n",
            "Sequence length:    30\n",
            "Feature dim:        5\n",
            "Mean |ITE|:         2.4961\n",
            "Std  ITE:           3.3482\n",
            "% near-zero ITE:    12.3%\n",
            "T0-T1 correlation:  0.9443\n",
            "=======================================================\n",
            "\n",
            "=======================================================\n",
            "Training: DCMVAE\n",
            "=======================================================\n",
            "  [MMD] z0=436, z1=524\n",
            "  Epoch 000 | loss=9.9192 | recon=0.9823 kl=0.0058 mmd=0.000000\n",
            "  [MMD] z0=472, z1=488\n",
            "  Epoch 030 | loss=9.9809 | recon=1.0341 kl=0.0360 mmd=0.000001\n",
            "  [MMD] z0=447, z1=513\n",
            "  Epoch 060 | loss=9.4974 | recon=0.8875 kl=0.0813 mmd=0.000001\n",
            "  [MMD] z0=528, z1=432\n",
            "  Epoch 090 | loss=9.9822 | recon=1.0465 kl=0.1524 mmd=0.000002\n",
            "  [MMD] z0=494, z1=466\n",
            "  Epoch 120 | loss=9.8183 | recon=0.9997 kl=0.1146 mmd=0.000001\n",
            "  [MMD] z0=498, z1=462\n",
            "  Epoch 149 | loss=9.8136 | recon=0.9955 kl=0.1261 mmd=0.000002\n",
            "\n",
            "=======================================================\n",
            "Training: Baseline1\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline1: 100%|██████████| 100/100 [00:11<00:00,  8.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: Baseline2\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline2: 100%|██████████| 100/100 [00:10<00:00,  9.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "Training: Baseline3\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline3: 100%|██████████| 100/100 [00:07<00:00, 14.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Model           | Test RMSE    | Test PEHE    \n",
            "----------------------------------------------------------------------\n",
            "DCMVAE          | 0.9880       | 3.0840       \n",
            "Baseline1       | 0.9790       | 3.1522       \n",
            "Baseline2       | 0.9931       | 3.2155       \n",
            "Baseline3       | 1.0017       | 3.1679       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}